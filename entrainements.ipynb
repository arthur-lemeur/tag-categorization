{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1167fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/laurent/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tags</th>\n",
       "      <th>Merged_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['c#']</td>\n",
       "      <td>convert decimal double c# want assign decimal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['html', 'css']</td>\n",
       "      <td>width collapse percentage width child element ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['c#', '.net']</td>\n",
       "      <td>calculate someone age base datetime type birth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['javascript', 'jquery', 'html', 'css']</td>\n",
       "      <td>jquery javascript find left inner edge element...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['c#']</td>\n",
       "      <td>calculate relative time c# give specific datet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Tags  \\\n",
       "0                                   ['c#']   \n",
       "1                          ['html', 'css']   \n",
       "2                           ['c#', '.net']   \n",
       "3  ['javascript', 'jquery', 'html', 'css']   \n",
       "4                                   ['c#']   \n",
       "\n",
       "                                          Merged_doc  \n",
       "0  convert decimal double c# want assign decimal ...  \n",
       "1  width collapse percentage width child element ...  \n",
       "2  calculate someone age base datetime type birth...  \n",
       "3  jquery javascript find left inner edge element...  \n",
       "4  calculate relative time c# give specific datet...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Cacher les warnings\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "data = pd.read_csv(\"data/cleaned_dataframe.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69499b89",
   "metadata": {},
   "source": [
    "# PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f91a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ LANCEMENT DE L'OPTIMISATION COMPL√àTE\n",
      "==================================================\n",
      "=== NETTOYAGE ET OPTIMISATION DU DATASET ===\n",
      "1. NETTOYAGE DES TAGS\n",
      "------------------------------\n",
      "Questions supprim√©es (sans tags): 16669\n",
      "Questions conserv√©es: 33331\n",
      "Pourcentage conserv√©: 66.7%\n",
      "\n",
      "2. DISTRIBUTION FINALE DES TAGS\n",
      "-----------------------------------\n",
      "Total de tags: 44552\n",
      "Tags uniques: 20\n",
      "Tags moyens par question: 1.34\n",
      "\n",
      "TOP 15 TAGS LES PLUS FR√âQUENTS:\n",
      " 1. javascript     :  5214 (15.6%)\n",
      " 2. python         :  5197 (15.6%)\n",
      " 3. java           :  4510 (13.5%)\n",
      " 4. c#             :  4346 (13.0%)\n",
      " 5. android        :  2829 ( 8.5%)\n",
      " 6. php            :  2527 ( 7.6%)\n",
      " 7. html           :  2523 ( 7.6%)\n",
      " 8. c++            :  2462 ( 7.4%)\n",
      " 9. ios            :  1767 ( 5.3%)\n",
      "10. .net           :  1741 ( 5.2%)\n",
      "11. jquery         :  1728 ( 5.2%)\n",
      "12. css            :  1655 ( 5.0%)\n",
      "13. sql            :  1316 ( 3.9%)\n",
      "14. node.js        :  1170 ( 3.5%)\n",
      "15. reactjs        :  1068 ( 3.2%)\n",
      "\n",
      "3. ANALYSE DES D√âS√âQUILIBRES\n",
      "--------------------------------\n",
      "Tag le plus rare: 806 exemples\n",
      "Tag le plus fr√©quent: 5214 exemples\n",
      "Ratio de d√©s√©quilibre: 6.5:1\n",
      "‚úÖ D√©s√©quilibre acceptable\n",
      "\n",
      "=== CR√âATION DE SOUS-ENSEMBLES √âQUILIBR√âS ===\n",
      "1. DATASET TOP 20 TAGS\n",
      "-------------------------\n",
      "Questions dans le top 20: 33331\n",
      "R√©duction: 0.0%\n",
      "\n",
      "2. DATASET TAGS √âQUILIBR√âS\n",
      "----------------------------\n",
      "Tags avec ‚â•500 exemples: 20\n",
      "Questions √©quilibr√©es: 33331\n",
      "R√©duction: 0.0%\n",
      "\n",
      "=== VERSION SINGLE-LABEL ===\n",
      "-----------------------------\n",
      "Questions avec tag primaire: 33331\n",
      "Tags primaires uniques: 20\n",
      "\n",
      "TOP 10 TAGS PRIMAIRES:\n",
      " 1. javascript     :  5214 (15.6%)\n",
      " 2. python         :  5157 (15.5%)\n",
      " 3. java           :  4433 (13.3%)\n",
      " 4. c#             :  4220 (12.7%)\n",
      " 5. c++            :  2310 ( 6.9%)\n",
      " 6. php            :  2219 ( 6.7%)\n",
      " 7. android        :  2191 ( 6.6%)\n",
      " 8. ios            :  1636 ( 4.9%)\n",
      " 9. html           :  1044 ( 3.1%)\n",
      "10. sql            :  1037 ( 3.1%)\n",
      "\n",
      "==================================================\n",
      "üìã R√âSUM√â DES DATASETS CR√â√âS:\n",
      "==================================================\n",
      "1. Dataset complet nettoy√©: 33,331 questions\n",
      "2. Dataset Top 20 tags: 33,331 questions\n",
      "3. Dataset √©quilibr√©: 33,331 questions\n",
      "4. Dataset single-label: 33,331 questions\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from typing import Union, List\n",
    "\n",
    "def clean_and_optimize_dataset(data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Nettoie et optimise le dataset\n",
    "    \"\"\"\n",
    "    print(\"=== NETTOYAGE ET OPTIMISATION DU DATASET ===\")\n",
    "    \n",
    "    # √âTAPE 1: Nettoyer les tags\n",
    "    print(\"1. NETTOYAGE DES TAGS\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    def parse_tags_safely(tags_str: Union[str, float]) -> List[str]:\n",
    "        \"\"\"Parse les tags depuis string vers liste\"\"\"\n",
    "        if pd.isna(tags_str):\n",
    "            return []\n",
    "        try:\n",
    "            # √âvaluer la string comme liste Python\n",
    "            tags_list = ast.literal_eval(tags_str)\n",
    "            return tags_list if isinstance(tags_list, list) else []\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    # Convertir tous les tags en listes\n",
    "    data['Tags_parsed'] = data['Tags'].apply(parse_tags_safely)\n",
    "    \n",
    "    # Supprimer les questions sans tags\n",
    "    data_clean = data[data['Tags_parsed'].apply(lambda x: len(x) > 0)].copy()\n",
    "    removed_count = len(data) - len(data_clean)\n",
    "    \n",
    "    print(f\"Questions supprim√©es (sans tags): {removed_count}\")\n",
    "    print(f\"Questions conserv√©es: {len(data_clean)}\")\n",
    "    print(f\"Pourcentage conserv√©: {len(data_clean)/len(data)*100:.1f}%\")\n",
    "    \n",
    "    # √âTAPE 2: Analyser la distribution finale\n",
    "    print(\"\\n2. DISTRIBUTION FINALE DES TAGS\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    all_tags = []\n",
    "    for tags in data_clean['Tags_parsed']:\n",
    "        all_tags.extend(tags)\n",
    "    \n",
    "    tag_distribution = Counter(all_tags)\n",
    "    print(f\"Total de tags: {len(all_tags)}\")\n",
    "    print(f\"Tags uniques: {len(tag_distribution)}\")\n",
    "    print(f\"Tags moyens par question: {len(all_tags)/len(data_clean):.2f}\")\n",
    "    \n",
    "    # Top 15 tags\n",
    "    print(f\"\\nTOP 15 TAGS LES PLUS FR√âQUENTS:\")\n",
    "    for i, (tag, count) in enumerate(tag_distribution.most_common(15), 1):\n",
    "        percentage = count / len(data_clean) * 100\n",
    "        print(f\"{i:2d}. {tag:15s}: {count:5d} ({percentage:4.1f}%)\")\n",
    "    \n",
    "    # √âTAPE 3: Identifier les d√©s√©quilibres critiques\n",
    "    print(\"\\n3. ANALYSE DES D√âS√âQUILIBRES\")\n",
    "    print(\"-\" * 32)\n",
    "    \n",
    "    tag_counts = list(tag_distribution.values())\n",
    "    min_count = min(tag_counts)\n",
    "    max_count = max(tag_counts)\n",
    "    ratio = max_count / min_count\n",
    "    \n",
    "    print(f\"Tag le plus rare: {min_count} exemples\")\n",
    "    print(f\"Tag le plus fr√©quent: {max_count} exemples\")\n",
    "    print(f\"Ratio de d√©s√©quilibre: {ratio:.1f}:1\")\n",
    "    \n",
    "    if ratio > 50:\n",
    "        print(\"üö® D√âS√âQUILIBRE CRITIQUE d√©tect√©!\")\n",
    "    elif ratio > 20:\n",
    "        print(\"‚ö†Ô∏è  D√âS√âQUILIBRE MOD√âR√â d√©tect√©\")\n",
    "    else:\n",
    "        print(\"‚úÖ D√©s√©quilibre acceptable\")\n",
    "    \n",
    "    return data_clean, tag_distribution\n",
    "\n",
    "def create_balanced_subsets(data_clean: pd.DataFrame, tag_distribution):\n",
    "    \"\"\"\n",
    "    Cr√©e des sous-ensembles √©quilibr√©s pour l'entra√Ænement\n",
    "    \"\"\"\n",
    "    print(\"\\n=== CR√âATION DE SOUS-ENSEMBLES √âQUILIBR√âS ===\")\n",
    "    \n",
    "    # STRAT√âGIE 1: Dataset avec tags les plus fr√©quents (Top 20)\n",
    "    print(\"1. DATASET TOP 20 TAGS\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    top_20_tags = set([tag for tag, _ in tag_distribution.most_common(20)])\n",
    "    \n",
    "    def has_top_tags(tags_list):\n",
    "        return any(tag in top_20_tags for tag in tags_list)\n",
    "    \n",
    "    data_top20 = data_clean[data_clean['Tags_parsed'].apply(has_top_tags)].copy()\n",
    "    \n",
    "    # Filtrer les tags pour ne garder que le top 20\n",
    "    def filter_to_top20(tags_list):\n",
    "        return [tag for tag in tags_list if tag in top_20_tags]\n",
    "    \n",
    "    data_top20['Tags_filtered'] = data_top20['Tags_parsed'].apply(filter_to_top20)\n",
    "    data_top20 = data_top20[data_top20['Tags_filtered'].apply(lambda x: len(x) > 0)]\n",
    "    \n",
    "    print(f\"Questions dans le top 20: {len(data_top20)}\")\n",
    "    print(f\"R√©duction: {(1 - len(data_top20)/len(data_clean))*100:.1f}%\")\n",
    "    \n",
    "    # STRAT√âGIE 2: Dataset avec tags √©quilibr√©s (min 500 exemples)\n",
    "    print(\"\\n2. DATASET TAGS √âQUILIBR√âS\")\n",
    "    print(\"-\" * 28)\n",
    "    \n",
    "    balanced_tags = set([tag for tag, count in tag_distribution.items() if count >= 500])\n",
    "    \n",
    "    def has_balanced_tags(tags_list):\n",
    "        return any(tag in balanced_tags for tag in tags_list)\n",
    "    \n",
    "    data_balanced = data_clean[data_clean['Tags_parsed'].apply(has_balanced_tags)].copy()\n",
    "    \n",
    "    def filter_to_balanced(tags_list):\n",
    "        return [tag for tag in tags_list if tag in balanced_tags]\n",
    "    \n",
    "    data_balanced['Tags_filtered'] = data_balanced['Tags_parsed'].apply(filter_to_balanced)\n",
    "    data_balanced = data_balanced[data_balanced['Tags_filtered'].apply(lambda x: len(x) > 0)]\n",
    "    \n",
    "    print(f\"Tags avec ‚â•500 exemples: {len(balanced_tags)}\")\n",
    "    print(f\"Questions √©quilibr√©es: {len(data_balanced)}\")\n",
    "    print(f\"R√©duction: {(1 - len(data_balanced)/len(data_clean))*100:.1f}%\")\n",
    "    \n",
    "    return data_top20, data_balanced\n",
    "\n",
    "# SCRIPT PRINCIPAL\n",
    "def main_optimization_pipeline(data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Pipeline complet d'optimisation\n",
    "    \"\"\"\n",
    "    print(\"üöÄ LANCEMENT DE L'OPTIMISATION COMPL√àTE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Nettoyage de base\n",
    "    data_clean, tag_distribution = clean_and_optimize_dataset(data)\n",
    "    \n",
    "    # 2. Cr√©ation de sous-ensembles\n",
    "    data_top20, data_balanced = create_balanced_subsets(data_clean, tag_distribution)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"üìã R√âSUM√â DES DATASETS CR√â√âS:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"1. Dataset complet nettoy√©: {len(data_clean):,} questions\")\n",
    "    print(f\"2. Dataset Top 20 tags: {len(data_top20):,} questions\")  \n",
    "    print(f\"3. Dataset √©quilibr√©: {len(data_balanced):,} questions\")\n",
    "    \n",
    "    return {\n",
    "        'clean': data_clean,\n",
    "        'top20': data_top20, \n",
    "        'balanced': data_balanced,\n",
    "        'tag_distribution': tag_distribution,\n",
    "    }\n",
    "\n",
    "# UTILISATION:\n",
    "results = main_optimization_pipeline(data)\n",
    "\n",
    "# Acc√©der aux datasets optimis√©s:\n",
    "data_clean = results['clean']\n",
    "data_top20 = results['top20'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d86c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probl√®mes identifi√©s et solutions\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, jaccard_score, recall_score, precision_score\n",
    "import ast\n",
    "import joblib\n",
    "\n",
    "def data_preparation(data_top20: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Version corrig√©e de la pr√©paration des donn√©es\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n=== PR√âPARATION CORRIG√âE DES DONN√âES ===\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # √âTAPE 1: NETTOYER ET PARSER LES TAGS EN PREMIER\n",
    "    print(\"1. NETTOYAGE ET PARSING DES TAGS\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    def parse_tags(tags):\n",
    "        \"\"\"Parse les tags de mani√®re robuste\"\"\"\n",
    "        if pd.isna(tags):\n",
    "            return []\n",
    "        if isinstance(tags, str):\n",
    "            try:\n",
    "                return ast.literal_eval(tags)\n",
    "            except:\n",
    "                return []\n",
    "        elif isinstance(tags, list):\n",
    "            return tags\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    # Nettoyer les donn√©es en amont\n",
    "    data_prepared = data_top20.copy()\n",
    "    data_prepared['Tags_parsed'] = data_prepared['Tags'].apply(parse_tags)\n",
    "    \n",
    "    # Supprimer les lignes sans tags\n",
    "    data_prepared = data_prepared[data_prepared['Tags_parsed'].apply(lambda x: len(x) > 0)]\n",
    "    \n",
    "    print(f\"Donn√©es apr√®s nettoyage: {len(data_prepared):,} questions\")\n",
    "    \n",
    "    # √âTAPE 2: BINARISATION POUR LA STRATIFICATION\n",
    "    print(\"\\n2. BINARISATION POUR STRATIFICATION\")\n",
    "    print(\"-\" * 37)\n",
    "    \n",
    "    # Pr√©parer X et y\n",
    "    X = data_prepared['Merged_doc'].values\n",
    "    y_tags = data_prepared['Tags_parsed'].values\n",
    "    \n",
    "    # Binariser AVANT le split pour permettre la stratification\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    y_binary = mlb.fit_transform(y_tags)\n",
    "    \n",
    "    print(f\"Features shape: {X.shape}\")\n",
    "    print(f\"Labels shape: {y_binary.shape}\")\n",
    "    print(f\"Nombre de classes: {len(mlb.classes_)}\")\n",
    "    print(f\"Classes: {list(mlb.classes_)[:10]}...\")  # Afficher les 10 premi√®res\n",
    "    \n",
    "    # √âTAPE 3: SPLIT STRATIFI√â MULTI-LABEL\n",
    "    print(\"\\n3. SPLIT STRATIFI√â MULTI-LABEL\")\n",
    "    print(\"-\" * 32)\n",
    "    \n",
    "    try:\n",
    "        # Essayer avec stratification multi-label\n",
    "        from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "        \n",
    "        splitter = MultilabelStratifiedShuffleSplit(\n",
    "            n_splits=1, \n",
    "            test_size=0.2, \n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        train_idx, test_idx = next(splitter.split(X, y_binary))\n",
    "        \n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train_bin, y_test_bin = y_binary[train_idx], y_binary[test_idx]\n",
    "        \n",
    "        print(\"‚úÖ Split stratifi√© multi-label r√©ussi\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è  iterstrat non disponible - split standard avec v√©rification\")\n",
    "        \n",
    "        # Fallback: split standard avec v√©rification manuelle\n",
    "        X_train, X_test, y_train_bin, y_test_bin = train_test_split(\n",
    "            X, y_binary,\n",
    "            test_size=0.2,\n",
    "            random_state=42,\n",
    "            # Utiliser une approximation de stratification\n",
    "            stratify=y_binary.sum(axis=1) > 1  # Stratifier par \"multi-tag\" vs \"single-tag\"\n",
    "        )\n",
    "    \n",
    "    print(f\"Training set: {len(X_train):,} √©chantillons\")\n",
    "    print(f\"Test set: {len(X_test):,} √©chantillons\")\n",
    "    \n",
    "    # √âTAPE 4: V√âRIFICATION DE LA DISTRIBUTION\n",
    "    print(\"\\n4. V√âRIFICATION DE LA DISTRIBUTION\")\n",
    "    print(\"-\" * 37)\n",
    "    \n",
    "    train_distribution = y_train_bin.sum(axis=0)\n",
    "    test_distribution = y_test_bin.sum(axis=0)\n",
    "    \n",
    "    print(\"Distribution des 10 premi√®res classes:\")\n",
    "    for i in range(min(10, len(mlb.classes_))):\n",
    "        class_name = mlb.classes_[i]\n",
    "        train_pct = train_distribution[i] / len(X_train) * 100\n",
    "        test_pct = test_distribution[i] / len(X_test) * 100\n",
    "        print(f\"  {class_name:12s}: train {train_pct:5.1f}% | test {test_pct:5.1f}% | ratio {train_pct/test_pct:.2f}\")\n",
    "    \n",
    "    # D√©tecter les classes probl√©matiques\n",
    "    problematic_classes = []\n",
    "    for i, class_name in enumerate(mlb.classes_):\n",
    "        if test_distribution[i] < 5:  # Moins de 5 exemples dans le test\n",
    "            problematic_classes.append((class_name, test_distribution[i]))\n",
    "    \n",
    "    if problematic_classes:\n",
    "        print(f\"\\n‚ö†Ô∏è  {len(problematic_classes)} classes avec <5 exemples dans le test:\")\n",
    "        for class_name, count in problematic_classes[:5]:\n",
    "            print(f\"     {class_name}: {count} exemples\")\n",
    "    \n",
    "    # √âTAPE 5: TF-IDF OPTIMIS√â\n",
    "    print(\"\\n5. VECTORISATION TF-IDF OPTIMIS√âE\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    # Configuration optimis√©e pour votre dataset\n",
    "    vocab_size = len(X_train)\n",
    "    min_df_optimal = max(5, min(50, vocab_size // 1000))  # Adaptatif\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        analyzer=\"word\",\n",
    "        min_df=min_df_optimal,      # Plus flexible que 100\n",
    "        max_df=0.95,                # Plus permissif que 0.8\n",
    "        max_features=10000,         # Limiter la dimensionnalit√©\n",
    "        token_pattern=r\"(?u)\\b\\w\\w+\\b\",  # Mots de 2+ caract√®res\n",
    "        ngram_range=(1, 2),         # Inclure bigrammes\n",
    "        stop_words='english',       # Supprimer stop words\n",
    "        sublinear_tf=True          # Am√©liore les performances\n",
    "    )\n",
    "    \n",
    "    print(f\"Configuration TF-IDF:\")\n",
    "    print(f\"  - min_df: {min_df_optimal}\")\n",
    "    print(f\"  - max_df: 0.95\")\n",
    "    print(f\"  - max_features: 10,000\")\n",
    "    print(f\"  - ngrams: (1,2)\")\n",
    "    \n",
    "    # Fit et transform\n",
    "    X_tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "    X_tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "    \n",
    "    print(f\"\\nR√©sultat TF-IDF:\")\n",
    "    print(f\"  Training: {X_tfidf_train.shape}\")\n",
    "    print(f\"  Test: {X_tfidf_test.shape}\")\n",
    "    print(f\"  Vocabulaire: {len(tfidf_vectorizer.get_feature_names_out())} mots\")\n",
    "    \n",
    "    # √âTAPE 6: M√âTRIQUES\n",
    "    print(\"\\n6. FONCTION DE M√âTRIQUES\")\n",
    "    print(\"-\" * 37)\n",
    "    \n",
    "    def metric_score(model_name, y_true, y_pred, metrics_df=None):\n",
    "        \"\"\"\n",
    "        Fonction de m√©triques corrig√©e pour multi-label\n",
    "        \"\"\"\n",
    "        \n",
    "        # M√©triques appropri√©es pour multi-label\n",
    "        results = {\n",
    "            'F1_macro': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "            'F1_micro': f1_score(y_true, y_pred, average='micro', zero_division=0),\n",
    "            'F1_weighted': f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "            'Precision_macro': precision_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "            'Recall_macro': recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "            'Jaccard_macro': jaccard_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "            'Subset_accuracy': accuracy_score(y_true, y_pred),  # Exact match\n",
    "        }\n",
    "        \n",
    "        # Calculer Hamming Loss (sp√©cifique multi-label)\n",
    "        from sklearn.metrics import hamming_loss\n",
    "        results['Hamming_loss'] = hamming_loss(y_true, y_pred)\n",
    "        \n",
    "        new_col = pd.DataFrame(results, index=[model_name]).T\n",
    "        \n",
    "        if metrics_df is None:\n",
    "            metrics_df = new_col\n",
    "        else:\n",
    "            metrics_df[model_name] = new_col[model_name]\n",
    "        \n",
    "        return metrics_df\n",
    "    \n",
    "    print(\"‚úÖ M√©triques multi-label configur√©es:\")\n",
    "    print(\"   - F1 macro/micro/weighted\")\n",
    "    print(\"   - Precision/Recall macro\") \n",
    "    print(\"   - Jaccard macro\")\n",
    "    print(\"   - Subset accuracy (exact match)\")\n",
    "    print(\"   - Hamming loss\")\n",
    "    \n",
    "    # √âTAPE 7: SAUVEGARDE S√âCURIS√âE\n",
    "    print(\"\\n7. SAUVEGARDE DES OBJETS\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    # Sauvegarder avec m√©tadonn√©es\n",
    "    objects_to_save = {\n",
    "        'tfidf_vectorizer': tfidf_vectorizer,\n",
    "        'mlb': mlb,\n",
    "        'feature_names': tfidf_vectorizer.get_feature_names_out(),\n",
    "        'classes': mlb.classes_,\n",
    "        'train_size': len(X_train),\n",
    "        'test_size': len(X_test),\n",
    "        'vocab_size': X_tfidf_train.shape[1]\n",
    "    }\n",
    "    \n",
    "    for name, obj in objects_to_save.items():\n",
    "        if name in ['feature_names', 'classes', 'train_size', 'test_size', 'vocab_size']:\n",
    "            continue  # Skip metadata\n",
    "        joblib.dump(obj, f'{name}_final.pkl')\n",
    "        print(f\"‚úÖ {name} sauvegard√©\")\n",
    "    \n",
    "    return {\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test, \n",
    "        'X_tfidf_train': X_tfidf_train,\n",
    "        'X_tfidf_test': X_tfidf_test,\n",
    "        'y_train_bin': y_train_bin,\n",
    "        'y_test_bin': y_test_bin,\n",
    "        'mlb': mlb,\n",
    "        'tfidf_vectorizer': tfidf_vectorizer,\n",
    "        'metric_function': metric_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2a2be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PR√âPARATION CORRIG√âE DES DONN√âES ===\n",
      "=============================================\n",
      "1. NETTOYAGE ET PARSING DES TAGS\n",
      "-----------------------------------\n",
      "Donn√©es apr√®s nettoyage: 33,331 questions\n",
      "\n",
      "2. BINARISATION POUR STRATIFICATION\n",
      "-------------------------------------\n",
      "Features shape: (33331,)\n",
      "Labels shape: (33331, 20)\n",
      "Nombre de classes: 20\n",
      "Classes: ['.net', 'android', 'arrays', 'asp.net', 'c#', 'c++', 'css', 'html', 'ios', 'java']...\n",
      "\n",
      "3. SPLIT STRATIFI√â MULTI-LABEL\n",
      "--------------------------------\n",
      "‚ö†Ô∏è  iterstrat non disponible - split standard avec v√©rification\n",
      "Training set: 26,664 √©chantillons\n",
      "Test set: 6,667 √©chantillons\n",
      "\n",
      "4. V√âRIFICATION DE LA DISTRIBUTION\n",
      "-------------------------------------\n",
      "Distribution des 10 premi√®res classes:\n",
      "  .net        : train   5.3% | test   5.1% | ratio 1.04\n",
      "  android     : train   8.2% | test   9.4% | ratio 0.87\n",
      "  arrays      : train   2.8% | test   2.7% | ratio 1.06\n",
      "  asp.net     : train   3.0% | test   2.9% | ratio 1.04\n",
      "  c#          : train  13.0% | test  13.1% | ratio 0.99\n",
      "  c++         : train   7.4% | test   7.3% | ratio 1.01\n",
      "  css         : train   4.9% | test   5.3% | ratio 0.93\n",
      "  html        : train   7.4% | test   8.2% | ratio 0.91\n",
      "  ios         : train   5.3% | test   5.2% | ratio 1.03\n",
      "  java        : train  13.5% | test  13.6% | ratio 1.00\n",
      "\n",
      "5. VECTORISATION TF-IDF OPTIMIS√âE\n",
      "-----------------------------------\n",
      "Configuration TF-IDF:\n",
      "  - min_df: 26\n",
      "  - max_df: 0.95\n",
      "  - max_features: 10,000\n",
      "  - ngrams: (1,2)\n",
      "\n",
      "R√©sultat TF-IDF:\n",
      "  Training: (26664, 10000)\n",
      "  Test: (6667, 10000)\n",
      "  Vocabulaire: 10000 mots\n",
      "\n",
      "6. FONCTION DE M√âTRIQUES CORRIG√âES\n",
      "-------------------------------------\n",
      "‚úÖ M√©triques multi-label configur√©es:\n",
      "   - F1 macro/micro/weighted\n",
      "   - Precision/Recall macro\n",
      "   - Jaccard macro\n",
      "   - Subset accuracy (exact match)\n",
      "   - Hamming loss\n",
      "\n",
      "7. SAUVEGARDE DES OBJETS\n",
      "-------------------------\n",
      "‚úÖ tfidf_vectorizer sauvegard√©\n",
      "‚úÖ mlb sauvegard√©\n"
     ]
    }
   ],
   "source": [
    "# 1. Pr√©parer les donn√©es\n",
    "prepared_data = data_preparation(data_top20)\n",
    "\n",
    "# 2. R√©cup√©rer les objets\n",
    "X_train = prepared_data['X_train']\n",
    "X_test = prepared_data['X_test']\n",
    "X_tfidf_train = prepared_data['X_tfidf_train']\n",
    "X_tfidf_test = prepared_data['X_tfidf_test'] \n",
    "Y_train_bin = prepared_data['y_train_bin']\n",
    "Y_test_bin = prepared_data['y_test_bin']\n",
    "mlb = prepared_data['mlb']\n",
    "metric_func = prepared_data['metric_function']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914638ed",
   "metadata": {},
   "source": [
    "# RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d846e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 18 candidates, totalling 36 fits\n",
      "--------------------------------------------------\n",
      "Best params for RandomForestClassifier\n",
      "--------------------------------------------------\n",
      "{'estimator__class_weight': 'balanced', 'estimator__max_depth': 50, 'estimator__min_samples_leaf': 5, 'estimator__n_estimators': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/04 17:12:55 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape des pr√©dictions: (6667, 20)\n",
      "Nombre de 1 dans les pr√©dictions: 10136\n",
      "Pourcentage de 1: 7.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/08/04 17:12:58 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Print 10 first predicted Tags vs true Tags\n",
      "--------------------------------------------------\n",
      "Predicted: [('c#',), ('arrays', 'javascript'), ('c#', 'sql', 'sql-server'), (), ('android', 'java'), ('arrays', 'python'), ('html', 'javascript', 'jquery'), ('python',), ('c#',), ('javascript', 'reactjs')]\n",
      "True: [('c#',), ('arrays', 'javascript'), ('c#', 'sql-server'), ('php',), ('java',), ('python',), ('javascript', 'jquery'), ('python',), ('c#',), ('javascript', 'reactjs')]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "mlflow.end_run()\n",
    "\n",
    "param_rfc = {\n",
    "    \"estimator__max_depth\": [5, 25, 50],\n",
    "    \"estimator__min_samples_leaf\": [1, 5, 10],\n",
    "    \"estimator__n_estimators\": [50, 100],\n",
    "    \"estimator__class_weight\": [\"balanced\"]\n",
    "}\n",
    "\n",
    "mlflow.set_experiment(\"Multilabel_TFIDF_RF_2\")\n",
    "f1_micro_scorer = make_scorer(f1_score, average='micro')\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "with mlflow.start_run(run_name=\"GridSearch_RF\", nested=True):\n",
    "\n",
    "    multi_rfc_cv = GridSearchCV(\n",
    "        OneVsRestClassifier(RandomForestClassifier()),\n",
    "        param_grid=param_rfc,\n",
    "        n_jobs=4,\n",
    "        cv=2,\n",
    "        scoring=f1_micro_scorer,\n",
    "        return_train_score=True,\n",
    "        refit=True,\n",
    "        verbose=3\n",
    "    )\n",
    "\n",
    "    multi_rfc_cv.fit(X_tfidf_train, Y_train_bin)\n",
    "\n",
    "    # Log des meilleurs hyperparam√®tres\n",
    "    mlflow.log_params(multi_rfc_cv.best_params_)\n",
    "\n",
    "    # R√©sultats de validation crois√©e\n",
    "    rfc_cv_results = pd.DataFrame.from_dict(multi_rfc_cv.cv_results_)\n",
    "    rfc_cv_results.to_csv(\"rfc_cv_results.csv\", index=False)\n",
    "    mlflow.log_artifact(\"rfc_cv_results.csv\")\n",
    "\n",
    "    print(\"-\"*50)\n",
    "    print(\"Best params for RandomForestClassifier\")\n",
    "    print(\"-\"*50)\n",
    "    print(multi_rfc_cv.best_params_)\n",
    "\n",
    "    y_test_predicted_labels_tfidf_rfc = multi_rfc_cv.predict(X_tfidf_test)\n",
    "\n",
    "    print(f\"Shape des pr√©dictions: {y_test_predicted_labels_tfidf_rfc.shape}\")\n",
    "    print(f\"Nombre de 1 dans les pr√©dictions: {y_test_predicted_labels_tfidf_rfc.sum()}\")\n",
    "    print(f\"Pourcentage de 1: {y_test_predicted_labels_tfidf_rfc.mean()*100:.2f}%\")\n",
    "\n",
    "    # M√©triques\n",
    "    metrics_df = metric_func(\"RF\", Y_test_bin, y_test_predicted_labels_tfidf_rfc)\n",
    "    mlflow.log_metric(\"accuracy\", metrics_df.loc[\"Subset_accuracy\", \"RF\"])\n",
    "    mlflow.log_metric(\"f1_score_macro\", metrics_df.loc[\"F1_macro\", \"RF\"])\n",
    "    mlflow.log_metric(\"f1_score_micro\", metrics_df.loc[\"F1_micro\", \"RF\"])\n",
    "    mlflow.log_metric(\"f1_score_weighted\", metrics_df.loc[\"F1_weighted\", \"RF\"])\n",
    "    mlflow.log_metric(\"jaccard\", metrics_df.loc[\"Jaccard_macro\", \"RF\"])\n",
    "    mlflow.log_metric(\"recall\", metrics_df.loc[\"Recall_macro\", \"RF\"])\n",
    "    mlflow.log_metric(\"precision\", metrics_df.loc[\"Precision_macro\", \"RF\"])\n",
    "\n",
    "    # Log du mod√®le\n",
    "    mlflow.sklearn.log_model(multi_rfc_cv.best_estimator_, name=\"model\")\n",
    "\n",
    "    # Affichage des 10 premi√®res pr√©dictions\n",
    "    y_test_pred_inversed = mlb.inverse_transform(y_test_predicted_labels_tfidf_rfc)\n",
    "    y_test_inversed = mlb.inverse_transform(Y_test_bin)\n",
    "\n",
    "    print(\"-\"*50)\n",
    "    print(\"Print 10 first predicted Tags vs true Tags\")\n",
    "    print(\"-\"*50)\n",
    "    print(\"Predicted:\", y_test_pred_inversed[:10])\n",
    "    print(\"True:\", y_test_inversed[:10])\n",
    "\n",
    "mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ab29eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1_macro</th>\n",
       "      <td>0.717541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1_micro</th>\n",
       "      <td>0.743528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1_weighted</th>\n",
       "      <td>0.751403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision_macro</th>\n",
       "      <td>0.685453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall_macro</th>\n",
       "      <td>0.763355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jaccard_macro</th>\n",
       "      <td>0.571757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subset_accuracy</th>\n",
       "      <td>0.496625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hamming_loss</th>\n",
       "      <td>0.036703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       RF\n",
       "F1_macro         0.717541\n",
       "F1_micro         0.743528\n",
       "F1_weighted      0.751403\n",
       "Precision_macro  0.685453\n",
       "Recall_macro     0.763355\n",
       "Jaccard_macro    0.571757\n",
       "Subset_accuracy  0.496625\n",
       "Hamming_loss     0.036703"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f40c10",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3768e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END estimator__class_weight=balanced, estimator__max_depth=5, estimator__min_samples_leaf=1, estimator__n_estimators=50;, score=(train=0.647, test=0.611) total time=   4.7s\n",
      "[CV 1/2] END estimator__class_weight=balanced, estimator__max_depth=5, estimator__min_samples_leaf=5, estimator__n_estimators=50;, score=(train=0.618, test=0.597) total time=   4.5s\n",
      "[CV 1/2] END estimator__class_weight=balanced, estimator__max_depth=5, estimator__min_samples_leaf=10, estimator__n_estimators=50;, score=(train=0.612, test=0.594) total time=   4.4s\n",
      "[CV 1/2] END estimator__class_weight=balanced, estimator__max_depth=5, estimator__min_samples_leaf=10, estimator__n_estimators=100;, score=(train=0.639, test=0.618) total time=   8.3s\n",
      "[CV 1/2] END estimator__class_weight=balanced, estimator__max_depth=25, estimator__min_samples_leaf=1, estimator__n_estimators=100;, score=(train=0.930, test=0.711) total time=  30.4s\n",
      "[CV 1/2] END estimator__class_weight=balanced, estimator__max_depth=25, estimator__min_samples_leaf=10, estimator__n_estimators=50;, score=(train=0.748, test=0.682) total time=  11.9s\n",
      "[CV 1/2] END estimator__class_weight=balanced, estimator__max_depth=25, estimator__min_samples_leaf=10, estimator__n_estimators=100;, score=(train=0.762, test=0.696) total time=  24.3s\n",
      "[CV 1/2] END estimator__class_weight=balanced, estimator__max_depth=50, estimator__min_samples_leaf=1, estimator__n_estimators=100;, score=(train=0.986, test=0.666) total time=  49.7s\n",
      "[CV 1/2] END estimator__class_weight=balanced, estimator__max_depth=50, estimator__min_samples_leaf=10, estimator__n_estimators=50;, score=(train=0.780, test=0.694) total time=  16.3s\n",
      "[CV 2/2] END estimator__class_weight=balanced, estimator__max_depth=5, estimator__min_samples_leaf=1, estimator__n_estimators=50;, score=(train=0.636, test=0.596) total time=   4.7s\n",
      "[CV 2/2] END estimator__class_weight=balanced, estimator__max_depth=5, estimator__min_samples_leaf=5, estimator__n_estimators=50;, score=(train=0.618, test=0.592) total time=   4.5s\n",
      "[CV 2/2] END estimator__class_weight=balanced, estimator__max_depth=5, estimator__min_samples_leaf=10, estimator__n_estimators=50;, score=(train=0.608, test=0.581) total time=   4.4s\n",
      "[CV 2/2] END estimator__class_weight=balanced, estimator__max_depth=5, estimator__min_samples_leaf=10, estimator__n_estimators=100;, score=(train=0.637, test=0.609) total time=   8.3s\n",
      "[CV 2/2] END estimator__class_weight=balanced, estimator__max_depth=25, estimator__min_samples_leaf=1, estimator__n_estimators=100;, score=(train=0.933, test=0.710) total time=  30.7s\n",
      "[CV 2/2] END estimator__class_weight=balanced, estimator__max_depth=25, estimator__min_samples_leaf=10, estimator__n_estimators=50;, score=(train=0.755, test=0.681) total time=  11.9s\n",
      "[CV 2/2] END estimator__class_weight=balanced, estimator__max_depth=25, estimator__min_samples_leaf=10, estimator__n_estimators=100;, score=(train=0.764, test=0.695) total time=  24.5s\n",
      "[CV 2/2] END estimator__class_weight=balanced, estimator__max_depth=50, estimator__min_samples_leaf=1, estimator__n_estimators=100;, score=(train=0.987, test=0.660) total time=  50.7s\n",
      "[CV 2/2] END estimator__class_weight=balanced, estimator__max_depth=50, estimator__min_samples_leaf=10, estimator__n_estimators=50;, score=(train=0.785, test=0.693) total time=  16.7s\n",
      "[CV 2/2] END estimator__class_weight=balanced, estimator__max_depth=5, estimator__min_samples_leaf=1, estimator__n_estimators=100;, score=(train=0.664, test=0.623) total time=   8.6s\n",
      "[CV 1/2] END estimator__class_weight=balanced, estimator__max_depth=5, estimator__min_samples_leaf=5, estimator__n_estimators=100;, score=(train=0.644, test=0.617) total time=   8.4s\n",
      "[CV 1/2] END estimator__class_weight=balanced, estimator__max_depth=25, estimator__min_samples_leaf=1, estimator__n_estimators=50;, score=(train=0.926, test=0.704) total time=  15.6s\n",
      "[CV 1/2] END estimator__class_weight=balanced, estimator__max_depth=25, estimator__min_samples_leaf=5, estimator__n_estimators=50;, score=(train=0.807, test=0.708) total time=  12.8s\n",
      "[CV 1/2] END estimator__class_weight=balanced, estimator__max_depth=25, estimator__min_samples_leaf=5, estimator__n_estimators=100;, score=(train=0.817, test=0.718) total time=  25.6s\n",
      "[CV 1/2] END estimator__class_weight=balanced, estimator__max_depth=50, estimator__min_samples_leaf=1, estimator__n_estimators=50;, score=(train=0.985, test=0.661) total time=  25.5s\n",
      "[CV 1/2] END estimator__class_weight=balanced, estimator__max_depth=50, estimator__min_samples_leaf=5, estimator__n_estimators=50;, score=(train=0.856, test=0.727) total time=  18.7s\n",
      "[CV 1/2] END estimator__class_weight=balanced, estimator__max_depth=50, estimator__min_samples_leaf=5, estimator__n_estimators=100;, score=(train=0.860, test=0.733) total time=  37.4s\n",
      "[CV 1/2] END estimator__class_weight=balanced, estimator__max_depth=50, estimator__min_samples_leaf=10, estimator__n_estimators=100;, score=(train=0.787, test=0.703) total time=  28.4s\n",
      "[CV 1/2] END estimator__class_weight=balanced, estimator__max_depth=5, estimator__min_samples_leaf=1, estimator__n_estimators=100;, score=(train=0.643, test=0.608) total time=   8.7s\n",
      "[CV 2/2] END estimator__class_weight=balanced, estimator__max_depth=5, estimator__min_samples_leaf=5, estimator__n_estimators=100;, score=(train=0.653, test=0.623) total time=   8.4s\n",
      "[CV 2/2] END estimator__class_weight=balanced, estimator__max_depth=25, estimator__min_samples_leaf=1, estimator__n_estimators=50;, score=(train=0.921, test=0.698) total time=  15.7s\n",
      "[CV 2/2] END estimator__class_weight=balanced, estimator__max_depth=25, estimator__min_samples_leaf=5, estimator__n_estimators=50;, score=(train=0.819, test=0.713) total time=  13.1s\n",
      "[CV 2/2] END estimator__class_weight=balanced, estimator__max_depth=25, estimator__min_samples_leaf=5, estimator__n_estimators=100;, score=(train=0.824, test=0.719) total time=  25.8s\n",
      "[CV 2/2] END estimator__class_weight=balanced, estimator__max_depth=50, estimator__min_samples_leaf=1, estimator__n_estimators=50;, score=(train=0.985, test=0.661) total time=  25.9s\n",
      "[CV 2/2] END estimator__class_weight=balanced, estimator__max_depth=50, estimator__min_samples_leaf=5, estimator__n_estimators=50;, score=(train=0.860, test=0.723) total time=  18.8s\n",
      "[CV 2/2] END estimator__class_weight=balanced, estimator__max_depth=50, estimator__min_samples_leaf=5, estimator__n_estimators=100;, score=(train=0.865, test=0.735) total time=  37.8s\n",
      "[CV 2/2] END estimator__class_weight=balanced, estimator__max_depth=50, estimator__min_samples_leaf=10, estimator__n_estimators=100;, score=(train=0.793, test=0.703) total time=  28.2s\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/04 17:14:13 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape des pr√©dictions: (6667, 20)\n",
      "Nombre de 1 dans les pr√©dictions: 10460\n",
      "Pourcentage de 1: 7.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/08/04 17:14:16 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [('.net', 'c#'), ('arrays', 'javascript'), ('c#', 'sql-server'), (), ('android',), ('python',), ('html', 'javascript', 'jquery'), ('python',), ('.net', 'c#'), ('javascript', 'reactjs')]\n",
      "True: [('c#',), ('arrays', 'javascript'), ('c#', 'sql-server'), ('php',), ('java',), ('python',), ('javascript', 'jquery'), ('python',), ('c#',), ('javascript', 'reactjs')]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import ast\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "mlflow.end_run()\n",
    "\n",
    "# Param√®tres pour la recherche\n",
    "param_logit = {\n",
    "    \"estimator__C\": [100, 10, 1.0, 0.1],\n",
    "    \"estimator__penalty\": [\"l1\", \"l2\"],\n",
    "    \"estimator__solver\": [\"liblinear\"],  # Bon compromis vitesse/convergence\n",
    "    \"estimator__class_weight\": [\"balanced\"],\n",
    "    \"estimator__tol\": [1e-4]  # Crit√®re de convergence moins strict\n",
    "}\n",
    "\n",
    "# D√©but du run MLflow\n",
    "mlflow.set_experiment(\"Multilabel_TFIDF_LogReg_2\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"GridSearch_LogReg\", nested=True):\n",
    "\n",
    "    # GridSearch\n",
    "    multi_logit_cv = GridSearchCV(\n",
    "        OneVsRestClassifier(LogisticRegression()),\n",
    "        param_grid=param_logit,\n",
    "        n_jobs=-1,\n",
    "        cv=5,\n",
    "        scoring=\"f1_weighted\",\n",
    "        return_train_score=True,\n",
    "        refit=True,\n",
    "        verbose=3\n",
    "    )\n",
    "    multi_logit_cv.fit(X_tfidf_train, Y_train_bin)\n",
    "\n",
    "    # Log des hyperparam√®tres optimaux\n",
    "    mlflow.log_params(multi_logit_cv.best_params_)\n",
    "\n",
    "    # Pr√©dictions\n",
    "    y_test_predicted_labels_tfidf = multi_logit_cv.predict(X_tfidf_test)\n",
    "\n",
    "    # √âvaluation\n",
    "    print(f\"Shape des pr√©dictions: {y_test_predicted_labels_tfidf.shape}\")\n",
    "    print(f\"Nombre de 1 dans les pr√©dictions: {y_test_predicted_labels_tfidf.sum()}\")\n",
    "    print(f\"Pourcentage de 1: {y_test_predicted_labels_tfidf.mean()*100:.2f}%\")\n",
    "\n",
    "    # M√©triques\n",
    "    metrics_df = metric_func(\"Log_Reg\", Y_test_bin, y_test_predicted_labels_tfidf, metrics_df)\n",
    "    mlflow.log_metric(\"accuracy\", metrics_df.loc[\"Subset_accuracy\", \"Log_Reg\"])\n",
    "    mlflow.log_metric(\"f1_score_macro\", metrics_df.loc[\"F1_macro\", \"Log_Reg\"])\n",
    "    mlflow.log_metric(\"f1_score_micro\", metrics_df.loc[\"F1_micro\", \"Log_Reg\"])\n",
    "    mlflow.log_metric(\"f1_score_weighted\", metrics_df.loc[\"F1_weighted\", \"Log_Reg\"])\n",
    "    mlflow.log_metric(\"jaccard\", metrics_df.loc[\"Jaccard_macro\", \"Log_Reg\"])\n",
    "    mlflow.log_metric(\"recall\", metrics_df.loc[\"Recall_macro\", \"Log_Reg\"])\n",
    "    mlflow.log_metric(\"precision\", metrics_df.loc[\"Precision_macro\", \"Log_Reg\"])\n",
    "\n",
    "    # Log du mod√®le\n",
    "    mlflow.sklearn.log_model(multi_logit_cv.best_estimator_, name=\"model\")\n",
    "\n",
    "    # Optionnel : log d'artefacts comme CSV\n",
    "    logit_cv_results = pd.DataFrame.from_dict(multi_logit_cv.cv_results_)\n",
    "    logit_cv_results.to_csv(\"logit_cv_results.csv\", index=False)\n",
    "    mlflow.log_artifact(\"logit_cv_results.csv\")\n",
    "\n",
    "    # Print comparaison pr√©dictions\n",
    "    y_test_pred_inversed = mlb.inverse_transform(y_test_predicted_labels_tfidf)\n",
    "    y_test_inversed = mlb.inverse_transform(Y_test_bin)\n",
    "    print(\"Predicted:\", y_test_pred_inversed[:10])\n",
    "    print(\"True:\", y_test_inversed[:10])\n",
    "\n",
    "mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b1b9cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RF</th>\n",
       "      <th>Log_Reg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1_macro</th>\n",
       "      <td>0.717541</td>\n",
       "      <td>0.731599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1_micro</th>\n",
       "      <td>0.743528</td>\n",
       "      <td>0.755849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1_weighted</th>\n",
       "      <td>0.751403</td>\n",
       "      <td>0.763459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision_macro</th>\n",
       "      <td>0.685453</td>\n",
       "      <td>0.678765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall_macro</th>\n",
       "      <td>0.763355</td>\n",
       "      <td>0.801186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jaccard_macro</th>\n",
       "      <td>0.571757</td>\n",
       "      <td>0.589822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subset_accuracy</th>\n",
       "      <td>0.496625</td>\n",
       "      <td>0.516724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hamming_loss</th>\n",
       "      <td>0.036703</td>\n",
       "      <td>0.035533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       RF   Log_Reg\n",
       "F1_macro         0.717541  0.731599\n",
       "F1_micro         0.743528  0.755849\n",
       "F1_weighted      0.751403  0.763459\n",
       "Precision_macro  0.685453  0.678765\n",
       "Recall_macro     0.763355  0.801186\n",
       "Jaccard_macro    0.571757  0.589822\n",
       "Subset_accuracy  0.496625  0.516724\n",
       "Hamming_loss     0.036703  0.035533"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4c6fb7",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e641b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D√©but de l'entra√Ænement SVM...\n",
      "Entra√Ænement en cours...\n",
      "[CV 3/5] END estimator__C=100, estimator__class_weight=balanced, estimator__penalty=l2, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=0.994, test=0.747) total time=  11.0s\n",
      "[CV 4/5] END estimator__C=100, estimator__class_weight=balanced, estimator__penalty=l2, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=0.994, test=0.747) total time=  10.6s\n",
      "[CV 2/5] END estimator__C=10, estimator__class_weight=balanced, estimator__penalty=l1, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=0.967, test=0.737) total time=  26.1s\n",
      "[CV 5/5] END estimator__C=1.0, estimator__class_weight=balanced, estimator__penalty=l1, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=0.805, test=0.739) total time=   9.0s\n",
      "[CV 4/5] END estimator__C=1.0, estimator__class_weight=balanced, estimator__penalty=l2, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=0.831, test=0.751) total time=   4.9s\n",
      "[CV 1/5] END estimator__C=0.1, estimator__class_weight=balanced, estimator__penalty=l2, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=0.753, test=0.721) total time=   2.8s\n",
      "[CV 2/5] END estimator__C=100, estimator__class_weight=balanced, estimator__penalty=l2, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=0.994, test=0.742) total time=  11.3s\n",
      "[CV 5/5] END estimator__C=100, estimator__class_weight=balanced, estimator__penalty=l2, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=0.994, test=0.736) total time=  10.5s\n",
      "[CV 3/5] END estimator__C=10, estimator__class_weight=balanced, estimator__penalty=l1, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=0.967, test=0.736) total time=  26.0s\n",
      "[CV 4/5] END estimator__C=1.0, estimator__class_weight=balanced, estimator__penalty=l1, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=0.804, test=0.737) total time=   9.3s\n",
      "[CV 1/5] END estimator__C=0.1, estimator__class_weight=balanced, estimator__penalty=l1, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=0.690, test=0.684) total time=   4.1s\n",
      "[CV 5/5] END estimator__C=0.1, estimator__class_weight=balanced, estimator__penalty=l1, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=0.692, test=0.684) total time=   3.8s\n",
      "[CV 5/5] END estimator__C=100, estimator__class_weight=balanced, estimator__penalty=l1, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=1.000, test=0.713) total time=  38.4s\n",
      "[CV 2/5] END estimator__C=10, estimator__class_weight=balanced, estimator__penalty=l2, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=0.924, test=0.765) total time=   7.8s\n",
      "[CV 1/5] END estimator__C=1.0, estimator__class_weight=balanced, estimator__penalty=l1, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=0.803, test=0.740) total time=   9.3s\n",
      "[CV 2/5] END estimator__C=1.0, estimator__class_weight=balanced, estimator__penalty=l2, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=0.831, test=0.753) total time=   4.9s\n",
      "[CV 3/5] END estimator__C=0.1, estimator__class_weight=balanced, estimator__penalty=l1, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=0.690, test=0.685) total time=   4.0s\n",
      "[CV 5/5] END estimator__C=0.1, estimator__class_weight=balanced, estimator__penalty=l2, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=0.753, test=0.720) total time=   1.6s\n",
      "[CV 1/5] END estimator__C=100, estimator__class_weight=balanced, estimator__penalty=l2, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=0.994, test=0.746) total time=  11.4s\n",
      "[CV 1/5] END estimator__C=10, estimator__class_weight=balanced, estimator__penalty=l1, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=0.968, test=0.735) total time=  25.3s\n",
      "[CV 4/5] END estimator__C=10, estimator__class_weight=balanced, estimator__penalty=l1, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=0.967, test=0.738) total time=  26.9s\n",
      "[CV 4/5] END estimator__C=0.1, estimator__class_weight=balanced, estimator__penalty=l2, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=0.752, test=0.722) total time=   2.2s\n",
      "[CV 3/5] END estimator__C=100, estimator__class_weight=balanced, estimator__penalty=l1, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=1.000, test=0.723) total time=  38.3s\n",
      "[CV 1/5] END estimator__C=10, estimator__class_weight=balanced, estimator__penalty=l2, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=0.923, test=0.766) total time=   7.9s\n",
      "[CV 5/5] END estimator__C=10, estimator__class_weight=balanced, estimator__penalty=l2, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=0.923, test=0.762) total time=   7.0s\n",
      "[CV 1/5] END estimator__C=1.0, estimator__class_weight=balanced, estimator__penalty=l2, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=0.832, test=0.753) total time=   4.7s\n",
      "[CV 2/5] END estimator__C=0.1, estimator__class_weight=balanced, estimator__penalty=l1, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=0.691, test=0.681) total time=   4.1s\n",
      "[CV 3/5] END estimator__C=0.1, estimator__class_weight=balanced, estimator__penalty=l2, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=0.753, test=0.720) total time=   2.8s\n",
      "[CV 2/5] END estimator__C=100, estimator__class_weight=balanced, estimator__penalty=l1, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=1.000, test=0.723) total time=  39.9s\n",
      "[CV 4/5] END estimator__C=10, estimator__class_weight=balanced, estimator__penalty=l2, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=0.924, test=0.764) total time=   7.7s\n",
      "[CV 3/5] END estimator__C=1.0, estimator__class_weight=balanced, estimator__penalty=l1, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=0.804, test=0.739) total time=   9.4s\n",
      "[CV 5/5] END estimator__C=1.0, estimator__class_weight=balanced, estimator__penalty=l2, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=0.832, test=0.753) total time=   4.9s\n",
      "[CV 2/5] END estimator__C=0.1, estimator__class_weight=balanced, estimator__penalty=l2, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=0.752, test=0.720) total time=   2.8s\n",
      "[CV 1/5] END estimator__C=100, estimator__class_weight=balanced, estimator__penalty=l1, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=1.000, test=0.723) total time=  38.0s\n",
      "[CV 5/5] END estimator__C=10, estimator__class_weight=balanced, estimator__penalty=l1, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=0.967, test=0.732) total time=  27.2s\n",
      "[CV 4/5] END estimator__C=100, estimator__class_weight=balanced, estimator__penalty=l1, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=1.000, test=0.721) total time=  38.9s\n",
      "[CV 3/5] END estimator__C=10, estimator__class_weight=balanced, estimator__penalty=l2, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=0.923, test=0.766) total time=   8.1s\n",
      "[CV 2/5] END estimator__C=1.0, estimator__class_weight=balanced, estimator__penalty=l1, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=0.804, test=0.743) total time=   9.1s\n",
      "[CV 3/5] END estimator__C=1.0, estimator__class_weight=balanced, estimator__penalty=l2, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=0.831, test=0.751) total time=   4.8s\n",
      "[CV 4/5] END estimator__C=0.1, estimator__class_weight=balanced, estimator__penalty=l1, estimator__solver=liblinear, estimator__tol=0.0001;, score=(train=0.690, test=0.685) total time=   3.8s\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/04 17:14:46 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Meilleurs param√®tres pour LinearSVC:\n",
      "--------------------------------------------------\n",
      "{'estimator__C': 1, 'estimator__class_weight': 'balanced', 'estimator__loss': 'squared_hinge', 'estimator__max_iter': 2000}\n",
      "Meilleur score CV: 0.7554\n",
      "Pr√©dictions en cours...\n",
      "Shape des pr√©dictions: (6667, 20)\n",
      "Nombre de 1 dans les pr√©dictions: 10220\n",
      "Pourcentage de 1: 7.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/08/04 17:14:48 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "R√âSULTATS SVM:\n",
      "--------------------------------------------------\n",
      "                       RF   Log_Reg       SVM\n",
      "F1_macro         0.717541  0.731599  0.727264\n",
      "F1_micro         0.743528  0.755849  0.750913\n",
      "F1_weighted      0.751403  0.763459  0.757796\n",
      "Precision_macro  0.685453  0.678765  0.689929\n",
      "Recall_macro     0.763355  0.801186  0.774880\n",
      "Jaccard_macro    0.571757  0.589822  0.584139\n",
      "Subset_accuracy  0.496625  0.516724  0.517324\n",
      "Hamming_loss     0.036703  0.035533  0.035803\n",
      "--------------------------------------------------\n",
      "10 premiers exemples - Tags pr√©dits vs vrais tags:\n",
      "--------------------------------------------------\n",
      "Exemple 1:\n",
      "  Pr√©dits: ('.net', 'c#')\n",
      "  Vrais:   ('c#',)\n",
      "\n",
      "Exemple 2:\n",
      "  Pr√©dits: ('arrays', 'javascript')\n",
      "  Vrais:   ('arrays', 'javascript')\n",
      "\n",
      "Exemple 3:\n",
      "  Pr√©dits: ('c#', 'sql-server')\n",
      "  Vrais:   ('c#', 'sql-server')\n",
      "\n",
      "Exemple 4:\n",
      "  Pr√©dits: ('node.js',)\n",
      "  Vrais:   ('php',)\n",
      "\n",
      "Exemple 5:\n",
      "  Pr√©dits: ('android',)\n",
      "  Vrais:   ('java',)\n",
      "\n",
      "Exemple 6:\n",
      "  Pr√©dits: ()\n",
      "  Vrais:   ('python',)\n",
      "\n",
      "Exemple 7:\n",
      "  Pr√©dits: ('html', 'javascript', 'jquery')\n",
      "  Vrais:   ('javascript', 'jquery')\n",
      "\n",
      "Exemple 8:\n",
      "  Pr√©dits: ('python',)\n",
      "  Vrais:   ('python',)\n",
      "\n",
      "Exemple 9:\n",
      "  Pr√©dits: ('.net', 'c#')\n",
      "  Vrais:   ('c#',)\n",
      "\n",
      "Exemple 10:\n",
      "  Pr√©dits: ('javascript', 'reactjs')\n",
      "  Vrais:   ('javascript', 'reactjs')\n",
      "\n",
      "Entra√Ænement SVM termin√©!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Fermer toute session MLflow active\n",
    "mlflow.end_run()\n",
    "\n",
    "# Param√®tres pour GridSearch SVM\n",
    "param_svc = {\n",
    "    \"estimator__C\": [0.1, 1, 10],  # Param√®tre de r√©gularisation\n",
    "    \"estimator__loss\": ['hinge', 'squared_hinge'],  # Type de loss\n",
    "    \"estimator__class_weight\": ['balanced'],  # Pour g√©rer le d√©s√©quilibre\n",
    "    \"estimator__max_iter\": [2000]  # Nombre max d'it√©rations\n",
    "}\n",
    "\n",
    "# Configuration MLflow\n",
    "mlflow.set_experiment(\"Multilabel_TFIDF_SVM_2\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"GridSearch_SVM\", nested=True):\n",
    "    \n",
    "    print(\"D√©but de l'entra√Ænement SVM...\")\n",
    "    \n",
    "    # Cr√©ation du mod√®le SVM multi-label\n",
    "    multi_svc_cv = GridSearchCV(\n",
    "        OneVsRestClassifier(LinearSVC(random_state=42, dual=False)),\n",
    "        param_grid=param_svc,\n",
    "        n_jobs=4,\n",
    "        cv=3,  # 3-fold cross validation\n",
    "        scoring=\"f1_weighted\",  # M√©trique optimis√©e\n",
    "        return_train_score=True,\n",
    "        refit=True,\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    # Entra√Ænement (utilisez vos donn√©es TF-IDF sans PCA pour de meilleurs r√©sultats)\n",
    "    print(\"Entra√Ænement en cours...\")\n",
    "    multi_svc_cv.fit(X_tfidf_train, Y_train_bin)  # Utilisation directe de TF-IDF sparse\n",
    "    \n",
    "    # Log des meilleurs hyperparam√®tres\n",
    "    mlflow.log_params(multi_svc_cv.best_params_)\n",
    "    \n",
    "    # Sauvegarde des r√©sultats de validation crois√©e\n",
    "    svc_cv_results = pd.DataFrame.from_dict(multi_svc_cv.cv_results_)\n",
    "    svc_cv_results.to_csv(\"svc_cv_results.csv\", index=False)\n",
    "    mlflow.log_artifact(\"svc_cv_results.csv\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(\"Meilleurs param√®tres pour LinearSVC:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(multi_svc_cv.best_params_)\n",
    "    print(f\"Meilleur score CV: {multi_svc_cv.best_score_:.4f}\")\n",
    "    \n",
    "    # Pr√©dictions sur le jeu de test\n",
    "    print(\"Pr√©dictions en cours...\")\n",
    "    y_test_predicted_labels_tfidf_svc = multi_svc_cv.predict(X_tfidf_test)\n",
    "    \n",
    "    print(f\"Shape des pr√©dictions: {y_test_predicted_labels_tfidf_svc.shape}\")\n",
    "    print(f\"Nombre de 1 dans les pr√©dictions: {y_test_predicted_labels_tfidf_svc.sum()}\")\n",
    "    print(f\"Pourcentage de 1: {y_test_predicted_labels_tfidf_svc.mean()*100:.2f}%\")\n",
    "    \n",
    "    \n",
    "    metrics_df_svm = metric_func(\"SVM\", Y_test_bin, y_test_predicted_labels_tfidf_svc, metrics_df=metrics_df)\n",
    "    mlflow.log_metric(\"accuracy\", metrics_df.loc[\"Subset_accuracy\", \"SVM\"])\n",
    "    mlflow.log_metric(\"f1_score_macro\", metrics_df.loc[\"F1_macro\", \"SVM\"])\n",
    "    mlflow.log_metric(\"f1_score_micro\", metrics_df.loc[\"F1_micro\", \"SVM\"])\n",
    "    mlflow.log_metric(\"f1_score_weighted\", metrics_df.loc[\"F1_weighted\", \"SVM\"])\n",
    "    mlflow.log_metric(\"jaccard\", metrics_df.loc[\"Jaccard_macro\", \"SVM\"])\n",
    "    mlflow.log_metric(\"recall\", metrics_df.loc[\"Recall_macro\", \"SVM\"])\n",
    "    mlflow.log_metric(\"precision\", metrics_df.loc[\"Precision_macro\", \"SVM\"])\n",
    "    \n",
    "    # Log du mod√®le\n",
    "    mlflow.sklearn.log_model(multi_logit_cv.best_estimator_, name=\"model\")\n",
    "    \n",
    "    # Affichage des r√©sultats\n",
    "    print(\"-\" * 50)\n",
    "    print(\"R√âSULTATS SVM:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(metrics_df_svm)\n",
    "    \n",
    "    # Comparaison des pr√©dictions avec la v√©rit√© terrain\n",
    "    y_test_pred_inversed_svm = mlb.inverse_transform(y_test_predicted_labels_tfidf_svc)\n",
    "    y_test_inversed = mlb.inverse_transform(Y_test_bin)\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(\"10 premiers exemples - Tags pr√©dits vs vrais tags:\")\n",
    "    print(\"-\" * 50)\n",
    "    for i in range(10):\n",
    "        print(f\"Exemple {i+1}:\")\n",
    "        print(f\"  Pr√©dits: {y_test_pred_inversed_svm[i]}\")\n",
    "        print(f\"  Vrais:   {y_test_inversed[i]}\")\n",
    "        print()\n",
    "\n",
    "print(\"Entra√Ænement SVM termin√©!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e48b004a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RF</th>\n",
       "      <th>Log_Reg</th>\n",
       "      <th>SVM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1_macro</th>\n",
       "      <td>0.717541</td>\n",
       "      <td>0.731599</td>\n",
       "      <td>0.727264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1_micro</th>\n",
       "      <td>0.743528</td>\n",
       "      <td>0.755849</td>\n",
       "      <td>0.750913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1_weighted</th>\n",
       "      <td>0.751403</td>\n",
       "      <td>0.763459</td>\n",
       "      <td>0.757796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision_macro</th>\n",
       "      <td>0.685453</td>\n",
       "      <td>0.678765</td>\n",
       "      <td>0.689929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall_macro</th>\n",
       "      <td>0.763355</td>\n",
       "      <td>0.801186</td>\n",
       "      <td>0.774880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jaccard_macro</th>\n",
       "      <td>0.571757</td>\n",
       "      <td>0.589822</td>\n",
       "      <td>0.584139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subset_accuracy</th>\n",
       "      <td>0.496625</td>\n",
       "      <td>0.516724</td>\n",
       "      <td>0.517324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hamming_loss</th>\n",
       "      <td>0.036703</td>\n",
       "      <td>0.035533</td>\n",
       "      <td>0.035803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       RF   Log_Reg       SVM\n",
       "F1_macro         0.717541  0.731599  0.727264\n",
       "F1_micro         0.743528  0.755849  0.750913\n",
       "F1_weighted      0.751403  0.763459  0.757796\n",
       "Precision_macro  0.685453  0.678765  0.689929\n",
       "Recall_macro     0.763355  0.801186  0.774880\n",
       "Jaccard_macro    0.571757  0.589822  0.584139\n",
       "Subset_accuracy  0.496625  0.516724  0.517324\n",
       "Hamming_loss     0.036703  0.035533  0.035803"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb30df28",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a54de2",
   "metadata": {},
   "source": [
    "# WORD2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e5642d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/04 17:15:32 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\u001b[31m2025/08/04 17:15:34 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import os\n",
    "import json\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Tokenisation\n",
    "X_train_tok = [word_tokenize(doc) for doc in X_train if pd.notna(doc)]\n",
    "X_test_tok = [word_tokenize(doc) for doc in X_test if pd.notna(doc)]\n",
    "\n",
    "# === D√©but du run MLflow ===\n",
    "mlflow.set_experiment(\"Word2Vec_Classifier\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"W2V + LogisticRegression\", nested=True):\n",
    "\n",
    "    # Hyperparams\n",
    "    vector_size = 100\n",
    "    window = 5\n",
    "    min_count = 2\n",
    "    sg = 1  # skip-gram\n",
    "\n",
    "    # Log des params\n",
    "    mlflow.log_params({\n",
    "        \"vector_size\": vector_size,\n",
    "        \"window\": window,\n",
    "        \"min_count\": min_count,\n",
    "        \"sg\": sg,\n",
    "        \"classifier\": \"LogisticRegression\"\n",
    "    })\n",
    "\n",
    "    # Entra√Ænement Word2Vec\n",
    "    model_w2v = Word2Vec(sentences=X_train_tok, vector_size=vector_size,\n",
    "                         window=window, min_count=min_count, workers=4, sg=sg)\n",
    "\n",
    "    # Sauvegarde du mod√®le Word2Vec\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        w2v_path = os.path.join(tmpdir, \"word2vec.model\")\n",
    "        model_w2v.save(w2v_path)\n",
    "        mlflow.log_artifact(w2v_path, artifact_path=\"w2v_model\")\n",
    "\n",
    "    # Vectorisation\n",
    "    def vectorize_doc(doc_tokens, model):\n",
    "        vectors = [model.wv[word] for word in doc_tokens if word in model.wv]\n",
    "        return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "    X_train_vec = np.array([vectorize_doc(doc, model_w2v) for doc in X_train_tok])\n",
    "    X_test_vec = np.array([vectorize_doc(doc, model_w2v) for doc in X_test_tok])\n",
    "\n",
    "    # Classification\n",
    "    clf = OneVsRestClassifier(LogisticRegression(max_iter=1000))\n",
    "    clf.fit(X_train_vec, Y_train_bin)\n",
    "\n",
    "    Y_pred_bin = clf.predict(X_test_vec)\n",
    "\n",
    "    # Logging mod√®le Sklearn\n",
    "    mlflow.sklearn.log_model(clf, name=\"model\")\n",
    "\n",
    "    # M√©triques\n",
    "    metrics_df = metric_func(\"Word2Vec\", Y_test_bin, Y_pred_bin, metrics_df)\n",
    "\n",
    "    mlflow.log_metric(\"accuracy\", metrics_df.loc[\"Subset_accuracy\", \"Word2Vec\"])\n",
    "    mlflow.log_metric(\"f1_score_macro\", metrics_df.loc[\"F1_macro\", \"Word2Vec\"])\n",
    "    mlflow.log_metric(\"f1_score_micro\", metrics_df.loc[\"F1_micro\", \"Word2Vec\"])\n",
    "    mlflow.log_metric(\"f1_score_weighted\", metrics_df.loc[\"F1_weighted\", \"Word2Vec\"])\n",
    "    mlflow.log_metric(\"jaccard\", metrics_df.loc[\"Jaccard_macro\", \"Word2Vec\"])\n",
    "    mlflow.log_metric(\"recall\", metrics_df.loc[\"Recall_macro\", \"Word2Vec\"])\n",
    "    mlflow.log_metric(\"precision\", metrics_df.loc[\"Precision_macro\", \"Word2Vec\"])\n",
    "\n",
    "mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8477643b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RF</th>\n",
       "      <th>Log_Reg</th>\n",
       "      <th>SVM</th>\n",
       "      <th>Word2Vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1_macro</th>\n",
       "      <td>0.717541</td>\n",
       "      <td>0.731599</td>\n",
       "      <td>0.727264</td>\n",
       "      <td>0.561472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1_micro</th>\n",
       "      <td>0.743528</td>\n",
       "      <td>0.755849</td>\n",
       "      <td>0.750913</td>\n",
       "      <td>0.642920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1_weighted</th>\n",
       "      <td>0.751403</td>\n",
       "      <td>0.763459</td>\n",
       "      <td>0.757796</td>\n",
       "      <td>0.629765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision_macro</th>\n",
       "      <td>0.685453</td>\n",
       "      <td>0.678765</td>\n",
       "      <td>0.689929</td>\n",
       "      <td>0.772534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall_macro</th>\n",
       "      <td>0.763355</td>\n",
       "      <td>0.801186</td>\n",
       "      <td>0.774880</td>\n",
       "      <td>0.456399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jaccard_macro</th>\n",
       "      <td>0.571757</td>\n",
       "      <td>0.589822</td>\n",
       "      <td>0.584139</td>\n",
       "      <td>0.410888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subset_accuracy</th>\n",
       "      <td>0.496625</td>\n",
       "      <td>0.516724</td>\n",
       "      <td>0.517324</td>\n",
       "      <td>0.430028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hamming_loss</th>\n",
       "      <td>0.036703</td>\n",
       "      <td>0.035533</td>\n",
       "      <td>0.035803</td>\n",
       "      <td>0.039696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       RF   Log_Reg       SVM  Word2Vec\n",
       "F1_macro         0.717541  0.731599  0.727264  0.561472\n",
       "F1_micro         0.743528  0.755849  0.750913  0.642920\n",
       "F1_weighted      0.751403  0.763459  0.757796  0.629765\n",
       "Precision_macro  0.685453  0.678765  0.689929  0.772534\n",
       "Recall_macro     0.763355  0.801186  0.774880  0.456399\n",
       "Jaccard_macro    0.571757  0.589822  0.584139  0.410888\n",
       "Subset_accuracy  0.496625  0.516724  0.517324  0.430028\n",
       "Hamming_loss     0.036703  0.035533  0.035803  0.039696"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b538d95",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "71be359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoConfig, TFAutoModel\n",
    "import time\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "73a863cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END estimator__C=0.1, estimator__class_weight=balanced, estimator__loss=hinge, estimator__max_iter=2000; total time=   0.0s\n",
      "[CV] END estimator__C=0.1, estimator__class_weight=balanced, estimator__loss=squared_hinge, estimator__max_iter=2000; total time=   2.4s\n",
      "[CV] END estimator__C=1, estimator__class_weight=balanced, estimator__loss=squared_hinge, estimator__max_iter=2000; total time=   3.9s\n",
      "[CV] END estimator__C=0.1, estimator__class_weight=balanced, estimator__loss=hinge, estimator__max_iter=2000; total time=   0.0s\n",
      "[CV] END estimator__C=0.1, estimator__class_weight=balanced, estimator__loss=squared_hinge, estimator__max_iter=2000; total time=   2.4s\n",
      "[CV] END estimator__C=10, estimator__class_weight=balanced, estimator__loss=hinge, estimator__max_iter=2000; total time=   0.0s\n",
      "[CV] END estimator__C=10, estimator__class_weight=balanced, estimator__loss=hinge, estimator__max_iter=2000; total time=   0.0s\n",
      "[CV] END estimator__C=10, estimator__class_weight=balanced, estimator__loss=hinge, estimator__max_iter=2000; total time=   0.0s\n",
      "[CV] END estimator__C=10, estimator__class_weight=balanced, estimator__loss=squared_hinge, estimator__max_iter=2000; total time=  19.3s\n",
      "[CV] END estimator__C=0.1, estimator__class_weight=balanced, estimator__loss=hinge, estimator__max_iter=2000; total time=   0.0s\n",
      "[CV] END estimator__C=1, estimator__class_weight=balanced, estimator__loss=hinge, estimator__max_iter=2000; total time=   0.0s\n",
      "[CV] END estimator__C=1, estimator__class_weight=balanced, estimator__loss=hinge, estimator__max_iter=2000; total time=   0.0s\n",
      "[CV] END estimator__C=1, estimator__class_weight=balanced, estimator__loss=hinge, estimator__max_iter=2000; total time=   0.0s\n",
      "[CV] END estimator__C=1, estimator__class_weight=balanced, estimator__loss=squared_hinge, estimator__max_iter=2000; total time=   4.0s\n",
      "[CV] END estimator__C=10, estimator__class_weight=balanced, estimator__loss=squared_hinge, estimator__max_iter=2000; total time=  18.6s\n",
      "[CV] END estimator__C=0.1, estimator__class_weight=balanced, estimator__loss=squared_hinge, estimator__max_iter=2000; total time=   2.4s\n",
      "[CV] END estimator__C=1, estimator__class_weight=balanced, estimator__loss=squared_hinge, estimator__max_iter=2000; total time=   3.8s\n",
      "[CV] END estimator__C=10, estimator__class_weight=balanced, estimator__loss=squared_hinge, estimator__max_iter=2000; total time=  18.0s\n"
     ]
    }
   ],
   "source": [
    "def bert_inp_fct(sentences, bert_tokenizer, max_length):\n",
    "    input_ids, token_type_ids, attention_mask = [], [], []\n",
    "    for sent in sentences:\n",
    "        bert_inp = bert_tokenizer.encode_plus(\n",
    "            sent,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"tf\"\n",
    "        )\n",
    "        input_ids.append(bert_inp['input_ids'][0])\n",
    "        token_type_ids.append(bert_inp['token_type_ids'][0])\n",
    "        attention_mask.append(bert_inp['attention_mask'][0])\n",
    "\n",
    "    return (\n",
    "        np.asarray(input_ids),\n",
    "        np.asarray(token_type_ids),\n",
    "        np.asarray(attention_mask)\n",
    "    )\n",
    "\n",
    "def feature_BERT_fct(model, tokenizer, sentences, max_length, batch_size):\n",
    "    all_embeddings = []\n",
    "    num_batches = (len(sentences) + batch_size - 1) // batch_size\n",
    "    for i in range(num_batches):\n",
    "        batch_sentences = sentences[i * batch_size : (i + 1) * batch_size]\n",
    "        input_ids, token_type_ids, attention_mask = bert_inp_fct(batch_sentences, tokenizer, max_length)\n",
    "\n",
    "        outputs = model([input_ids, attention_mask, token_type_ids], training=False)\n",
    "        last_hidden_state = outputs.last_hidden_state.numpy()  # (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        # Moyenne sur la s√©quence\n",
    "        mean_embeddings = np.mean(last_hidden_state, axis=1)\n",
    "        all_embeddings.append(mean_embeddings)\n",
    "\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "# === Mod√®le BERT HF ===\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "bert_model = TFAutoModel.from_pretrained(model_name)\n",
    "\n",
    "max_length = 32\n",
    "batch_size = 10\n",
    "\n",
    "# === Extraction des features BERT ===\n",
    "X_train_bert = feature_BERT_fct(bert_model, tokenizer, X_train, max_length, batch_size)\n",
    "X_test_bert = feature_BERT_fct(bert_model, tokenizer, X_test, max_length, batch_size)\n",
    "\n",
    "# === R√©duction de dimension ===\n",
    "pca = PCA(n_components=100, random_state=42)\n",
    "X_train_pca = pca.fit_transform(X_train_bert)\n",
    "X_test_pca = pca.transform(X_test_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19f8eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/04 17:33:56 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[31m2025/08/04 17:33:59 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RF</th>\n",
       "      <th>Log_Reg</th>\n",
       "      <th>SVM</th>\n",
       "      <th>Word2Vec</th>\n",
       "      <th>Bert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1_macro</th>\n",
       "      <td>0.717541</td>\n",
       "      <td>0.731599</td>\n",
       "      <td>0.727264</td>\n",
       "      <td>0.561472</td>\n",
       "      <td>0.561472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1_micro</th>\n",
       "      <td>0.743528</td>\n",
       "      <td>0.755849</td>\n",
       "      <td>0.750913</td>\n",
       "      <td>0.642920</td>\n",
       "      <td>0.642920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1_weighted</th>\n",
       "      <td>0.751403</td>\n",
       "      <td>0.763459</td>\n",
       "      <td>0.757796</td>\n",
       "      <td>0.629765</td>\n",
       "      <td>0.629765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision_macro</th>\n",
       "      <td>0.685453</td>\n",
       "      <td>0.678765</td>\n",
       "      <td>0.689929</td>\n",
       "      <td>0.772534</td>\n",
       "      <td>0.772534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall_macro</th>\n",
       "      <td>0.763355</td>\n",
       "      <td>0.801186</td>\n",
       "      <td>0.774880</td>\n",
       "      <td>0.456399</td>\n",
       "      <td>0.456399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jaccard_macro</th>\n",
       "      <td>0.571757</td>\n",
       "      <td>0.589822</td>\n",
       "      <td>0.584139</td>\n",
       "      <td>0.410888</td>\n",
       "      <td>0.410888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subset_accuracy</th>\n",
       "      <td>0.496625</td>\n",
       "      <td>0.516724</td>\n",
       "      <td>0.517324</td>\n",
       "      <td>0.430028</td>\n",
       "      <td>0.430028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hamming_loss</th>\n",
       "      <td>0.036703</td>\n",
       "      <td>0.035533</td>\n",
       "      <td>0.035803</td>\n",
       "      <td>0.039696</td>\n",
       "      <td>0.039696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       RF   Log_Reg       SVM  Word2Vec      Bert\n",
       "F1_macro         0.717541  0.731599  0.727264  0.561472  0.561472\n",
       "F1_micro         0.743528  0.755849  0.750913  0.642920  0.642920\n",
       "F1_weighted      0.751403  0.763459  0.757796  0.629765  0.629765\n",
       "Precision_macro  0.685453  0.678765  0.689929  0.772534  0.772534\n",
       "Recall_macro     0.763355  0.801186  0.774880  0.456399  0.456399\n",
       "Jaccard_macro    0.571757  0.589822  0.584139  0.410888  0.410888\n",
       "Subset_accuracy  0.496625  0.516724  0.517324  0.430028  0.430028\n",
       "Hamming_loss     0.036703  0.035533  0.035803  0.039696  0.039696"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "mlflow.set_experiment(\"Multilabel_BERT\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"LogReg_BERT\", nested=True):\n",
    "    clf = OneVsRestClassifier(LogisticRegression(max_iter=1000))\n",
    "    clf.fit(X_train_pca, Y_train_bin)\n",
    "\n",
    "    # === Pr√©dictions ===\n",
    "    y_pred = clf.predict(X_test_pca)\n",
    "    # Metrics\n",
    "    metrics_df = metric_func(\"Bert\", Y_test_bin, Y_pred_bin, metrics_df)\n",
    "\n",
    "    model_type = 'bert-base-uncased'\n",
    "    # Log params\n",
    "    mlflow.log_param(\"BERT_model\", model_type)\n",
    "    mlflow.log_param(\"max_length\", max_length)\n",
    "    mlflow.log_param(\"pca_components\", 100)\n",
    "\n",
    "    # Log metrics (juste apr√®s calcul)\n",
    "    mlflow.log_metric(\"accuracy\", metrics_df.loc[\"Subset_accuracy\", \"Bert\"])\n",
    "    mlflow.log_metric(\"f1_score_macro\", metrics_df.loc[\"F1_macro\", \"Bert\"])\n",
    "    mlflow.log_metric(\"f1_score_micro\", metrics_df.loc[\"F1_micro\", \"Bert\"])\n",
    "    mlflow.log_metric(\"f1_score_weighted\", metrics_df.loc[\"F1_weighted\", \"Bert\"])\n",
    "    mlflow.log_metric(\"jaccard\", metrics_df.loc[\"Jaccard_macro\", \"Bert\"])\n",
    "    mlflow.log_metric(\"recall\", metrics_df.loc[\"Recall_macro\", \"Bert\"])\n",
    "    mlflow.log_metric(\"precision\", metrics_df.loc[\"Precision_macro\", \"Bert\"])\n",
    "\n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(clf, name=\"model\")\n",
    "\n",
    "mlflow.end_run()\n",
    "\n",
    "# Affichage final\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d77f31e",
   "metadata": {},
   "source": [
    "# USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9ce8b2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# D√©finir le cache temporaire\n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = \"/tmp/tfhub_cache\"\n",
    "\n",
    "# Charger le mod√®le USE\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eb6aa6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_USE_fct(sentences, batch_size):\n",
    "    features = []\n",
    "    time1 = time.time()\n",
    "\n",
    "    for step in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[step:step + batch_size]\n",
    "\n",
    "        # üîΩ Correction ici : aplatir si array 2D\n",
    "        if isinstance(batch, np.ndarray):\n",
    "            batch = batch.ravel().tolist()\n",
    "\n",
    "        feat = embed(batch)\n",
    "        features.append(feat.numpy())\n",
    "\n",
    "    features = np.vstack(features)\n",
    "    print(f\"Temps d'encodage : {np.round(time.time() - time1, 2)} sec\")\n",
    "\n",
    "    return features\n",
    "\n",
    "X_train = np.ravel(X_train).tolist()\n",
    "X_test = np.ravel(X_test).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bea035a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def display_scree_plot(pca):\n",
    "  fig=plt.figure(figsize=(8,8))\n",
    "  scree = pca.explained_variance_ratio_*100\n",
    "  plt.bar(np.arange(len(scree))+1, scree)\n",
    "  plt.plot(np.arange(len(scree))+1, scree.cumsum(),c=\"red\",marker='o')\n",
    "  plt.xlabel(\"rang de l'axe d'inertie\")\n",
    "  plt.ylabel(\"pourcentage d'inertie\")\n",
    "  plt.title(\"Eboulis des valeurs propres\")\n",
    "  plt.show()\n",
    "\n",
    "def pca_transformation(train , test):\n",
    "    scaler = StandardScaler()\n",
    "    train = scaler.fit_transform(train)\n",
    "    test = scaler.transform(test)\n",
    "    n_comp = train.shape[1]\n",
    "    pca = PCA(n_components=n_comp)\n",
    "    pca.fit(train)\n",
    "    display_scree_plot(pca)\n",
    "    pca = PCA(n_components=0.8, random_state=42)\n",
    "    pca.fit(train)\n",
    "    train_pca = pca.transform(train)\n",
    "    test_pca = pca.transform(test)\n",
    "    print(\"\\nNous conservons {} composantes principales pour garder 80% d'inertie\".format(pca.components_.shape[0]))\n",
    "    return train_pca, test_pca, pca\n",
    "\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "313cbeb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'encodage : 9.99 sec\n",
      "Temps d'encodage : 2.42 sec\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nous conservons 131 composantes principales pour garder 80% d'inertie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2025/08/04 17:38:06 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[31m2025/08/04 17:38:08 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RF</th>\n",
       "      <th>Log_Reg</th>\n",
       "      <th>SVM</th>\n",
       "      <th>Word2Vec</th>\n",
       "      <th>Bert</th>\n",
       "      <th>USE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1_macro</th>\n",
       "      <td>0.717541</td>\n",
       "      <td>0.731599</td>\n",
       "      <td>0.727264</td>\n",
       "      <td>0.561472</td>\n",
       "      <td>0.561472</td>\n",
       "      <td>0.672210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1_micro</th>\n",
       "      <td>0.743528</td>\n",
       "      <td>0.755849</td>\n",
       "      <td>0.750913</td>\n",
       "      <td>0.642920</td>\n",
       "      <td>0.642920</td>\n",
       "      <td>0.725258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1_weighted</th>\n",
       "      <td>0.751403</td>\n",
       "      <td>0.763459</td>\n",
       "      <td>0.757796</td>\n",
       "      <td>0.629765</td>\n",
       "      <td>0.629765</td>\n",
       "      <td>0.717887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision_macro</th>\n",
       "      <td>0.685453</td>\n",
       "      <td>0.678765</td>\n",
       "      <td>0.689929</td>\n",
       "      <td>0.772534</td>\n",
       "      <td>0.772534</td>\n",
       "      <td>0.777378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall_macro</th>\n",
       "      <td>0.763355</td>\n",
       "      <td>0.801186</td>\n",
       "      <td>0.774880</td>\n",
       "      <td>0.456399</td>\n",
       "      <td>0.456399</td>\n",
       "      <td>0.598919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jaccard_macro</th>\n",
       "      <td>0.571757</td>\n",
       "      <td>0.589822</td>\n",
       "      <td>0.584139</td>\n",
       "      <td>0.410888</td>\n",
       "      <td>0.410888</td>\n",
       "      <td>0.522264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subset_accuracy</th>\n",
       "      <td>0.496625</td>\n",
       "      <td>0.516724</td>\n",
       "      <td>0.517324</td>\n",
       "      <td>0.430028</td>\n",
       "      <td>0.430028</td>\n",
       "      <td>0.524224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hamming_loss</th>\n",
       "      <td>0.036703</td>\n",
       "      <td>0.035533</td>\n",
       "      <td>0.035803</td>\n",
       "      <td>0.039696</td>\n",
       "      <td>0.039696</td>\n",
       "      <td>0.033291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       RF   Log_Reg       SVM  Word2Vec      Bert       USE\n",
       "F1_macro         0.717541  0.731599  0.727264  0.561472  0.561472  0.672210\n",
       "F1_micro         0.743528  0.755849  0.750913  0.642920  0.642920  0.725258\n",
       "F1_weighted      0.751403  0.763459  0.757796  0.629765  0.629765  0.717887\n",
       "Precision_macro  0.685453  0.678765  0.689929  0.772534  0.772534  0.777378\n",
       "Recall_macro     0.763355  0.801186  0.774880  0.456399  0.456399  0.598919\n",
       "Jaccard_macro    0.571757  0.589822  0.584139  0.410888  0.410888  0.522264\n",
       "Subset_accuracy  0.496625  0.516724  0.517324  0.430028  0.430028  0.524224\n",
       "Hamming_loss     0.036703  0.035533  0.035803  0.039696  0.039696  0.033291"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"Multilabel_USE\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"LogReg_USE\", nested=True):\n",
    "    # Encodage avec USE\n",
    "    X_use_train = feature_USE_fct(X_train, batch_size)\n",
    "    X_use_test = feature_USE_fct(X_test, batch_size)\n",
    "\n",
    "    # PCA\n",
    "    X_train_use_pca, X_test_use_pca, pca_use = pca_transformation(X_use_train, X_use_test)\n",
    "    n_components = pca_use.n_components_\n",
    "\n",
    "    # Log des param√®tres\n",
    "    mlflow.log_param(\"embedding\", \"USE\")\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"pca_components\", n_components)\n",
    "    mlflow.log_param(\"C\", 10)\n",
    "    mlflow.log_param(\"penalty\", \"l1\")\n",
    "    mlflow.log_param(\"solver\", \"liblinear\")\n",
    "\n",
    "    # Mod√®le\n",
    "    logit_use_pca = OneVsRestClassifier(\n",
    "        LogisticRegression(C=10, penalty=\"l1\", dual=False, solver=\"liblinear\"),\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    logit_use_pca.fit(X_train_use_pca, Y_train_bin)\n",
    "\n",
    "    # Pr√©dictions\n",
    "    Y_pred_use = logit_use_pca.predict(X_test_use_pca)\n",
    "\n",
    "    metrics_df = metric_func(\"USE\", Y_test_bin, Y_pred_use, metrics_df)\n",
    "\n",
    "    # Log m√©triques\n",
    "    mlflow.log_metric(\"accuracy\", metrics_df.loc[\"Subset_accuracy\", \"USE\"])\n",
    "    mlflow.log_metric(\"f1_score_macro\", metrics_df.loc[\"F1_macro\", \"USE\"])\n",
    "    mlflow.log_metric(\"f1_score_micro\", metrics_df.loc[\"F1_micro\", \"USE\"])\n",
    "    mlflow.log_metric(\"f1_score_weighted\", metrics_df.loc[\"F1_weighted\", \"USE\"])\n",
    "    mlflow.log_metric(\"jaccard\", metrics_df.loc[\"Jaccard_macro\", \"USE\"])\n",
    "    mlflow.log_metric(\"recall\", metrics_df.loc[\"Recall_macro\", \"USE\"])\n",
    "    mlflow.log_metric(\"precision\", metrics_df.loc[\"Precision_macro\", \"USE\"])\n",
    "\n",
    "    # Log du mod√®le\n",
    "    mlflow.sklearn.log_model(logit_use_pca, \"model\")\n",
    "\n",
    "mlflow.end_run()\n",
    "\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a36d33-0d1d-438b-935f-d4b5fb935706",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
