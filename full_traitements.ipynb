{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2bafe52",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "e2bafe52",
    "outputId": "cbda1482-81e6-48ad-e19e-b665fc7fdb2a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/laurent/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tags</th>\n",
       "      <th>Merged_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['c#']</td>\n",
       "      <td>convert decimal double c# want assign decimal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['html', 'css']</td>\n",
       "      <td>width collapse percentage width child element ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['c#', '.net']</td>\n",
       "      <td>calculate someone age base datetime type birth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['javascript', 'jquery', 'html', 'css']</td>\n",
       "      <td>jquery javascript find left inner edge element...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['c#']</td>\n",
       "      <td>calculate relative time c# give specific datet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Tags  \\\n",
       "0                                   ['c#']   \n",
       "1                          ['html', 'css']   \n",
       "2                           ['c#', '.net']   \n",
       "3  ['javascript', 'jquery', 'html', 'css']   \n",
       "4                                   ['c#']   \n",
       "\n",
       "                                          Merged_doc  \n",
       "0  convert decimal double c# want assign decimal ...  \n",
       "1  width collapse percentage width child element ...  \n",
       "2  calculate someone age base datetime type birth...  \n",
       "3  jquery javascript find left inner edge element...  \n",
       "4  calculate relative time c# give specific datet...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Cacher les warnings\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "data = pd.read_csv(\"data/cleaned_dataframe.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11c2849f",
   "metadata": {
    "id": "11c2849f"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.drop('Tags', axis=1)  # features\n",
    "y = data['Tags']               # label\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f69c39ae",
   "metadata": {
    "id": "f69c39ae"
   },
   "outputs": [],
   "source": [
    "corpus_tokenized = data['Merged_doc'].dropna().apply(word_tokenize).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3621620",
   "metadata": {
    "id": "c3621620"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import ast\n",
    "\n",
    "# Binarisation des tags\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y_train_bin = mlb.fit_transform(y_train.apply(ast.literal_eval))\n",
    "Y_test_bin = mlb.transform(y_test.apply(ast.literal_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "o6PLT751M2PQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o6PLT751M2PQ",
    "outputId": "a28ec62e-1ba8-4d8d-e500-0e36304bdd00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_test_bin: <class 'numpy.ndarray'> (15000, 50)\n"
     ]
    }
   ],
   "source": [
    "print(\"Y_test_bin:\", type(Y_test_bin), Y_test_bin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "121bacbe",
   "metadata": {
    "id": "121bacbe"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, jaccard_score, recall_score, precision_score\n",
    "\n",
    "model_comparaison_metrics = pd.DataFrame()\n",
    "\n",
    "def metric_score(model_name, Y_test_bin, Y_pred_bin, metrics_df=None):\n",
    "    results = {\n",
    "        'Accuracy': accuracy_score(Y_test_bin, Y_pred_bin),\n",
    "        'F1 score': f1_score(Y_test_bin, Y_pred_bin, average='macro'),\n",
    "        'Jaccard': jaccard_score(Y_test_bin, Y_pred_bin, average='macro'),\n",
    "        'Recall': recall_score(Y_test_bin, Y_pred_bin, average='macro'),\n",
    "        'Precision': precision_score(Y_test_bin, Y_pred_bin, average='macro')\n",
    "    }\n",
    "\n",
    "    new_col = pd.DataFrame(results, index=[model_name]).T\n",
    "\n",
    "    if metrics_df is None:\n",
    "        metrics_df = new_col\n",
    "    else:\n",
    "        metrics_df[model_name] = new_col[model_name]\n",
    "\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5VokjPM5CU4y",
   "metadata": {
    "id": "5VokjPM5CU4y"
   },
   "source": [
    "# COUNT-VECTORIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "q1WuPAWNCTfq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q1WuPAWNCTfq",
    "outputId": "d868b03f-5601-4bcd-e3b2-66ee89dd84f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulaire:  {'create': 213, 'use': 939, 'new': 589, 'go': 396, 'lot': 521, 'question': 709, 'com': 167, 'find': 355, 'solution': 813, 'requirement': 744, 'post': 674, 'dynamic': 287, 'property': 696, 'column': 165, 'name': 579, 'set': 792, 'value': 949, 'bind': 94, 'data': 226, 'follow': 368, 'step': 839, 'achieve': 5, 'correct': 203, 'wrong': 991, 'contains': 195, 'row': 759, 'class': 150, 'add': 12, 'make': 526, 'list': 506, 'do': 273, 'public': 702, 'foreach': 372, 'col': 162, 'var': 950, 'string': 846, 'object': 608, 'tostring': 907, 'else': 295, 'execute': 325, 'loop': 520, 'contain': 193, 'last': 485, 'please': 662, 'help': 415, 'fix': 361, 'center': 136, 'image': 432, 'look': 519, 'like': 501, 'document': 276, 'function': 382, 'customer': 223, 'true': 918, 'false': 344, 'setting': 793, 'body': 99, 'background': 80, 'color': 164, 'text': 884, 'align': 21, 'pad': 629, 'px': 705, 'margin': 532, 'img': 433, 'width': 976, 'position': 672, 'display': 268, 'block': 97, 'box': 108, 'size': 810, 'border': 106, 'user': 940, 'select': 782, 'none': 595, 'action': 6, 'hidden': 417, 'focus': 366, 'cursor': 221, 'pointer': 669, 'track': 911, 'transform': 914, 'top': 905, 'leave': 492, 'table': 872, 'content': 196, 'clear': 153, 'load': 508, 'hide': 418, 'float': 364, 'height': 413, 'min': 558, 'dir': 263, 'right': 754, 'event': 315, 'initialize': 443, 'auto': 69, 'script': 773, 'src': 828, 'type': 924, 'javascript': 464, 'link': 504, 'href': 424, 'div': 270, 'container': 194, 'br': 109, 'section': 778, 'style': 851, 'tried': 916, 'cs': 215, 'get': 389, 'want': 964, 'pretty': 677, 'much': 573, 'know': 478, 'wonder': 982, 'way': 966, 'standard': 832, 'sure': 864, 'request': 742, 'configuration': 182, 'asp': 56, 'net': 586, 'application': 42, 'server': 788, 'come': 168, 'put': 704, 'process': 687, 'never': 588, 'multiple': 574, 'time': 897, 'since': 807, 'could': 205, 'finish': 357, 'job': 467, 'different': 261, 'order': 621, 'one': 614, 'call': 123, 'understand': 930, 'possible': 673, 'control': 198, 'number': 605, 'thread': 894, 'able': 0, 'result': 751, 'would': 987, 'level': 497, 'even': 314, 'accept': 1, 'response': 749, 'think': 892, 'suppose': 863, 'detect': 251, 'window': 978, 'service': 789, 'database': 227, 'thing': 891, 'depend': 244, 'record': 723, 'azure': 77, 'automatically': 70, 'deploy': 246, 'need': 585, 'stop': 841, 'tell': 879, 'whether': 972, 'release': 733, 'pipeline': 656, 'command': 169, 'task': 876, 'update': 935, 'code': 161, 'static': 836, 'program': 691, 'void': 961, 'main': 525, 'run': 762, 'private': 684, 'engine': 304, 'protect': 697, 'override': 627, 'args': 48, 'try': 919, 'catch': 132, 'exception': 322, 'ex': 318, 'logger': 515, 'error': 312, 'fail': 343, 'null': 603, 'return': 753, 'sender': 785, 'switch': 866, 'case': 128, 'change': 139, 'trace': 910, 'filename': 351, 'break': 111, 'message': 552, 'refresh': 729, 'back': 78, 'screen': 772, 'click': 155, 'work': 984, 'compose': 179, 'implement': 434, 'feature': 346, 'however': 423, 'issue': 460, 'face': 340, 'scroll': 774, 'behavior': 89, 'fun': 380, 'bundle': 118, 'super': 861, 'define': 240, 'route': 757, 'icon': 427, 'val': 945, 'profile': 690, 'fill': 352, 'label': 479, 'onclick': 613, 'graph': 401, 'id': 428, 'modifier': 567, 'api': 38, 'launch': 487, 'effect': 292, 'start': 833, 'unit': 932, 'delay': 242, 'network': 587, 'reset': 746, 'state': 834, 'framework': 377, 'express': 332, 'problem': 685, 'access': 2, 'large': 484, 'app': 39, 'parent': 636, 'child': 146, 'many': 530, 'specific': 822, 'choose': 147, 'item': 461, 'drop': 284, 'another': 33, 'first': 359, 'collection': 163, 'session': 791, 'ui': 925, 'store': 843, 'take': 874, 'second': 777, 'sql': 827, 'long': 518, 'wait': 963, 'two': 922, 'simple': 805, 'yes': 997, 'say': 766, 'quite': 711, 'force': 371, 'enter': 305, 'filter': 353, 'see': 780, 'remove': 735, 'import': 436, 'file': 350, 'project': 692, 'without': 981, 'search': 776, 'answer': 34, 'open': 615, 'press': 676, 'sort': 817, 'rule': 761, 'query': 708, 'int': 451, 'db': 233, 'print': 681, 'information': 440, 'console': 187, 'produce': 688, 'thanks': 889, 'suggestion': 858, 'generate': 387, 'option': 619, 'sum': 859, 'view': 957, 'line': 503, 'per': 648, 'every': 316, 'total': 908, 'model': 566, 'camera': 125, 'give': 393, 'occur': 609, 'base': 83, 'sample': 764, 'google': 399, 'basic': 84, 'always': 25, 'support': 862, 'non': 594, 'though': 893, 'trigger': 917, 'builder': 117, 'mode': 565, 'also': 24, 'device': 256, 'integer': 452, 'log': 514, 'tag': 873, 'require': 743, 'django': 271, 'group': 404, 'hour': 422, 'day': 232, 'py': 706, 'default': 239, 'max': 540, 'length': 494, 'date': 230, 'price': 679, 'weight': 970, 'plot': 663, 'timestamp': 899, 'edit': 291, 'dataset': 229, 'month': 570, 'str': 844, 'variable': 952, 'connection': 185, 'happen': 409, 'python': 707, 'send': 784, 'echo': 289, 'url': 938, 'org': 622, 'site': 809, 'input': 444, 'already': 23, 'detail': 250, 'save': 765, 'full': 379, 'note': 597, 'socket': 812, 'config': 181, 'except': 321, 'key': 476, 'def': 238, 'root': 756, 'hello': 414, 'method': 555, 'global': 395, 'json': 472, 'close': 157, 'receive': 722, 'host': 421, 'port': 671, 'undefined': 929, 'react': 717, 'array': 51, 'native': 582, 'environment': 309, 'system': 870, 'throw': 896, 'show': 801, 'nothing': 598, 'solve': 814, 'node': 593, 'module': 569, 'register': 732, 'cause': 134, 'entry': 307, 'path': 644, 'role': 755, 'mvc': 576, 'reason': 721, 'controller': 199, 'developer': 254, 'std': 838, 'range': 714, 'element': 294, 'consider': 186, 'point': 668, 'vector': 953, 'something': 816, 'documentation': 277, 'mention': 549, 'assume': 60, 'structure': 848, 'seem': 781, 'miss': 560, 'double': 279, 'template': 881, 'const': 188, 'namespace': 580, 'struct': 847, 'constant': 189, 'still': 840, 'maybe': 542, 'extract': 338, 'generic': 388, 'fit': 360, 'example': 319, 'spring': 825, 'test': 883, 'flutter': 365, 'card': 127, 'widget': 975, 'wrap': 988, 'next': 590, 'easy': 288, 'copy': 201, 'source': 818, 'target': 875, 'machine': 523, 'mysql': 577, 'good': 398, 'extra': 337, 'upload': 936, 'field': 348, 'product': 689, 'backend': 79, 'page': 630, 'custom': 222, 'meta': 553, 'plugin': 665, 'additional': 13, 'attach': 62, 'chart': 143, 'matrix': 538, 'idea': 429, 'download': 280, 'high': 419, 'convert': 200, 'browser': 112, 'far': 345, 'bit': 95, 'approach': 45, 'either': 293, 'embed': 297, 'html': 425, 'js': 471, 'anything': 36, 'chrome': 148, 'xml': 993, 'resource': 748, 'well': 971, 'expect': 329, 'perform': 649, 'compile': 175, 'instead': 450, 'initial': 442, 'cache': 121, 'must': 575, 'invoke': 457, 'handler': 408, 'really': 720, 'manually': 529, 'check': 144, 'windows': 979, 'title': 900, 'algorithm': 20, 'tab': 871, 'active': 7, 'correctly': 204, 'promise': 693, 'figure': 349, 'advance': 16, 'keep': 474, 'syntax': 868, 'read': 718, 'include': 437, 'msg': 572, 'virtual': 958, 'char': 141, 'write': 990, 'visual': 960, 'studio': 849, 'asset': 58, 'aws': 74, 'rail': 712, 'exec': 324, 'push': 703, 'origin': 623, 'final': 354, 'docker': 275, 'present': 675, 'everything': 317, 'build': 116, 'ask': 55, 'white': 973, 'space': 819, 'layout': 489, 'fragment': 375, 'version': 954, 'encode': 301, 'utf': 943, 'android': 27, 'design': 248, 'xmlns': 994, 'tool': 904, 'match': 536, 'bar': 82, 'blue': 98, 'bean': 87, 'package': 628, 'might': 557, 'directory': 266, 'home': 420, 'crash': 212, 'separate': 786, 'address': 14, 'memory': 548, 'map': 531, 'library': 500, 'lib': 499, 'warn': 965, 'available': 71, 'replace': 738, 'domain': 278, 'na': 578, 'book': 100, 'core': 202, 'enable': 300, 'normal': 596, 'exit': 328, 'place': 657, 'kind': 477, 'firebase': 358, 'security': 779, 'account': 4, 'delete': 243, 'auth': 67, 'validate': 947, 'city': 149, 'inside': 446, 'directly': 265, 'yet': 998, 'web': 967, 'implementation': 435, 'someone': 815, 'remote': 734, 'via': 955, 'java': 463, 'provide': 698, 'connect': 184, 'attempt': 63, 'rest': 750, 'println': 683, 'lang': 481, 'sun': 860, 'client': 156, 'init': 441, 'format': 374, 'language': 482, 'declare': 237, 'currently': 220, 'develop': 253, 'sheet': 798, 'apps': 46, 'difference': 260, 'parameter': 634, 'component': 178, 'retrieve': 752, 'foo': 370, 'mean': 545, 'pass': 642, 'platform': 659, 'best': 90, 'team': 878, 'microsoft': 556, 'runtime': 763, 'win': 977, 'fine': 356, 'button': 119, 'end': 302, 'train': 912, 'single': 808, 'whole': 974, 'io': 458, 'annotation': 32, 'notice': 599, 'self': 783, 'nil': 592, 'png': 667, 'frame': 376, 'animation': 31, 'bool': 101, 'amount': 26, 'submit': 854, 'scala': 767, 'stream': 845, 'local': 510, 'cluster': 159, 'jar': 462, 'apache': 37, 'configure': 183, 'lambda': 480, 'util': 944, 'unknown': 933, 'worker': 985, 'reflect': 728, 'common': 172, 'resolve': 747, 'obj': 607, 'boost': 103, 'several': 795, 'printf': 682, 'counter': 207, 'cout': 210, 'buffer': 114, 'part': 639, 'cloud': 158, 'req': 741, 'status': 837, 'append': 41, 'ignore': 431, 'real': 719, 'equal': 310, 'attribute': 65, 'doc': 274, 'servlet': 790, 'http': 426, 'javax': 465, 'catalina': 131, 'tomcat': 903, 'comment': 170, 'param': 633, 'fetch': 347, 'successfully': 856, 'interface': 454, 'age': 17, 'extend': 334, 'activity': 8, 'arraylist': 52, 'let': 496, 'appreciate': 44, 'disable': 267, 'instal': 447, 'reference': 727, 'low': 522, 'properly': 695, 'php': 654, 'curl': 218, 'login': 517, 'website': 969, 'txt': 923, 'header': 412, 'token': 902, 'email': 296, 'username': 941, 'password': 643, 'timeout': 898, 'form': 373, 'ssl': 830, 'logic': 516, 'res': 745, 'output': 626, 'len': 493, 'sign': 803, 'count': 206, 'num': 604, 'alert': 19, 'location': 512, 'index': 438, 'font': 369, 'placeholder': 658, 'span': 820, 'boot': 104, 'accord': 3, 'within': 980, 'router': 758, 'env': 308, 'uri': 937, 'err': 311, 're': 716, 'render': 736, 'category': 133, 'description': 247, 'npm': 602, 'dependency': 245, 'three': 895, 'thank': 888, 'debug': 236, 'current': 219, 'groupid': 405, 'springframework': 826, 'artifactid': 54, 'scope': 770, 'around': 50, 'invalid': 456, 'similar': 804, 'dll': 272, 'anyone': 35, 'setup': 794, 'apply': 43, 'etc': 313, 'handle': 407, 'dictionary': 259, 'parse': 637, 'parser': 638, 'argument': 49, 'course': 209, 'actually': 10, 'goal': 397, 'mobile': 562, 'validation': 948, 'stack': 831, 'certain': 137, 'csv': 217, 'minute': 559, 'guess': 406, 'info': 439, 'split': 824, 'sub': 852, 'join': 468, 'pas': 641, 'ng': 591, 'angular': 30, 'basically': 85, 'allow': 22, 'certificate': 138, 'instance': 449, 'ip': 459, 'ok': 611, 'play': 660, 'turn': 920, 'schema': 769, 'oracle': 620, 'excel': 320, 'provider': 699, 'exist': 327, 'factory': 342, 'context': 197, 'menu': 550, 'article': 53, 'notification': 600, 'entity': 306, 'specify': 823, 'flag': 362, 'boolean': 102, 'layer': 488, 'complete': 177, 'repository': 740, 'may': 541, 'term': 882, 'head': 411, 'person': 652, 'columns': 166, 'begin': 88, 'scale': 768, 'bad': 81, 'folder': 367, 'dataframe': 228, 'character': 142, 'cat': 130, 'cli': 154, 'linux': 505, 'shell': 799, 'previous': 678, 'gcc': 385, 'os': 625, 'np': 601, 'word': 983, 'df': 257, 'axis': 76, 'nan': 581, 'shape': 796, 'execution': 326, 'redirect': 725, 'video': 956, 'md': 544, 'particular': 640, 'statement': 835, 'unsigned': 934, 'performance': 650, 'share': 797, 'permission': 651, 'cmd': 160, 'ax': 75, 'plt': 664, 'modify': 568, 'red': 724, 'simply': 806, 'draw': 282, 'pool': 670, 'appear': 40, 'explain': 330, 'wrapper': 989, 'bin': 92, 'learn': 490, 'topic': 906, 'tf': 886, 'pattern': 645, 'offset': 610, 'random': 713, 'getelementbyid': 390, 'unable': 928, 'temp': 880, 'avoid': 72, 'move': 571, 'datetime': 231, 'internal': 455, 'endpoint': 303, 'authentication': 68, 'valid': 946, 'calculate': 122, 'branch': 110, 'batch': 86, 'func': 381, 'primary': 680, 'metadata': 554, 'subject': 853, 'mail': 524, 'gl': 394, 'uint': 926, 'constraint': 190, 'compiler': 176, 'suggest': 857, 'unique': 931, 'install': 448, 'usr': 942, 'mb': 543, 'dist': 269, 'repeat': 737, 'async': 61, 'sys': 869, 'assembly': 57, 'de': 235, 'ref': 726, 'due': 285, 'manager': 528, 'plugins': 666, 'channel': 140, 'xcode': 992, 'small': 811, 'big': 91, 'original': 624, 'ruby': 760, 'clean': 152, 'cast': 129, 'tr': 909, 'th': 887, 'td': 877, 'laravel': 483, 'transaction': 913, 'manage': 527, 'export': 331, 'short': 800, 'ie': 430, 'rather': 715, 'cell': 135, 'dim': 262, 'empty': 298, 'cpp': 211, 'extension': 335, 'jpg': 469, 'member': 547, 'world': 986, 'constructor': 191, 'medium': 546, 'success': 855, 'development': 255, 'localhost': 511, 'external': 336, 'opt': 618, 'hash': 410, 'exe': 323, 'ad': 11, 'dd': 234, 'github': 392, 'git': 391, 'callback': 124, 'direction': 264, 'condition': 180, 'black': 96, 'ajax': 18, 'li': 498, 'fa': 339, 'score': 771, 'storage': 842, 'little': 507, 'insert': 445, 'await': 73, 'less': 495, 'game': 384, 'eclipse': 290, 'visible': 959, 'bottom': 107, 'driver': 283, 'kernel': 475, 'late': 486, 'company': 173, 'definition': 241, 'side': 802, 'numpy': 606, 'mm': 561, 'lock': 513, 'merge': 551, 'tmp': 901, 'gradle': 400, 'dev': 252, 'hibernate': 416, 'grid': 403, 'pd': 646, 'regex': 730, 'modal': 564, 'navigation': 584, 'audio': 66, 'phone': 653, 'spark': 821, 'master': 534, 'actual': 9, 'intent': 453, 'loader': 509, 'pair': 631, 'binary': 93, 'operator': 617, 'expression': 333, 'stuff': 850, 'bug': 115, 'great': 402, 'css': 216, 'ul': 927, 'panel': 632, 'player': 661, 'desktop': 249, 'limit': 502, 'theme': 890, 'old': 612, 'byte': 120, 'country': 208, 'admin': 15, 'flex': 363, 'zero': 999, 'androidx': 29, 'textview': 885, 'tree': 915, 'params': 635, 'canvas': 126, 'math': 537, 'region': 731, 'compare': 174, 'credential': 214, 'free': 378, 'sequence': 787, 'operation': 616, 'year': 996, 'assign': 59, 'vue': 962, 'future': 383, 'btn': 113, 'contact': 192, 'sdk': 775, 'duplicate': 286, 'prop': 694, 'procedure': 686, 'attr': 64, 'dialog': 258, 'en': 299, 'report': 739, 'nav': 583, 'classname': 151, 'bootstrap': 105, 'jdbc': 466, 'androidruntime': 28, 'facebook': 341, 'queue': 710, 'area': 47, 'least': 491, 'jquery': 470, 'ssh': 829, 'tutorial': 921, 'symbol': 867, 'pip': 655, 'pdf': 647, 'commit': 171, 'mock': 563, 'checkbox': 145, 'varchar': 951, 'maven': 539, 'ptr': 701, 'junit': 473, 'cv': 224, 'webpack': 968, 'proxy': 700, 'dp': 281, 'marker': 533, 'dart': 225, 'svg': 865, 'gem': 386, 'xsl': 995, 'mat': 535}\n",
      "Feature Matrix (train):\n",
      " [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=1000, max_df=0.8, min_df=2)\n",
    "\n",
    "# Apprentissage du vocabulaire sur X_train uniquement\n",
    "X_train_vect = vectorizer.fit_transform(X_train['Merged_doc'])\n",
    "\n",
    "# Transformation de X_test avec le même vectorizer\n",
    "X_test_vect = vectorizer.transform(X_test['Merged_doc'])\n",
    "\n",
    "# Affichage\n",
    "print(\"Vocabulaire: \", vectorizer.vocabulary_)\n",
    "print(\"Feature Matrix (train):\\n\", X_train_vect.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "F2mRu1BUCZK_",
   "metadata": {
    "id": "F2mRu1BUCZK_"
   },
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdpL-NY4Cbg3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fdpL-NY4Cbg3",
    "outputId": "e73d3b6b-5bf2-423c-9316-2ba221a64c1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le jeu d'entrainement comporte 35000 documents pour 120018 mots\n",
      "Le jeu de Test comporte 15000 documents pour 120018 mots\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "\n",
    "tfidf_vectorizer =  TfidfVectorizer(analyzer=\"word\", \n",
    "    min_df=5,  # Plus permissif\n",
    "    max_df=0.8,  # Ignorer les mots trop fréquents\n",
    "    ngram_range=(1, 2),\n",
    "    )\n",
    "\n",
    "# create TF-IDF features\n",
    "X_tfidf_train = tfidf_vectorizer.fit_transform(X_train['Merged_doc'])\n",
    "X_tfidf_test = tfidf_vectorizer.transform(X_test['Merged_doc'])\n",
    "feature_names_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer_2.pkl')\n",
    "\n",
    "print(\"Le jeu d'entrainement comporte {} documents pour {} mots\".format(X_tfidf_train.shape[0], X_tfidf_train.shape[1]))\n",
    "print(\"Le jeu de Test comporte {} documents pour {} mots\".format(X_tfidf_test.shape[0], X_tfidf_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bFGDKQHNChbD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "bFGDKQHNChbD",
    "outputId": "2bc03bd4-ada2-45f6-fcff-03f4c4de009a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aa aa</th>\n",
       "      <th>aa ab</th>\n",
       "      <th>aa bb</th>\n",
       "      <th>aa ce</th>\n",
       "      <th>aa dd</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaa aaa</th>\n",
       "      <th>aaa bbb</th>\n",
       "      <th>aaaa</th>\n",
       "      <th>...</th>\n",
       "      <th>zx</th>\n",
       "      <th>zxing</th>\n",
       "      <th>zygoteinit</th>\n",
       "      <th>zygoteinit java</th>\n",
       "      <th>zygoteinit main</th>\n",
       "      <th>zygoteinit methodandargscaller</th>\n",
       "      <th>zz</th>\n",
       "      <th>zza</th>\n",
       "      <th>zzb</th>\n",
       "      <th>zzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 120018 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aa  aa aa  aa ab  aa bb  aa ce  aa dd  aaa  aaa aaa  aaa bbb  aaaa  ...  \\\n",
       "0  0.0    0.0    0.0    0.0    0.0    0.0  0.0      0.0      0.0   0.0  ...   \n",
       "1  0.0    0.0    0.0    0.0    0.0    0.0  0.0      0.0      0.0   0.0  ...   \n",
       "2  0.0    0.0    0.0    0.0    0.0    0.0  0.0      0.0      0.0   0.0  ...   \n",
       "3  0.0    0.0    0.0    0.0    0.0    0.0  0.0      0.0      0.0   0.0  ...   \n",
       "4  0.0    0.0    0.0    0.0    0.0    0.0  0.0      0.0      0.0   0.0  ...   \n",
       "\n",
       "    zx  zxing  zygoteinit  zygoteinit java  zygoteinit main  \\\n",
       "0  0.0    0.0         0.0              0.0              0.0   \n",
       "1  0.0    0.0         0.0              0.0              0.0   \n",
       "2  0.0    0.0         0.0              0.0              0.0   \n",
       "3  0.0    0.0         0.0              0.0              0.0   \n",
       "4  0.0    0.0         0.0              0.0              0.0   \n",
       "\n",
       "   zygoteinit methodandargscaller   zz  zza  zzb  zzz  \n",
       "0                             0.0  0.0  0.0  0.0  0.0  \n",
       "1                             0.0  0.0  0.0  0.0  0.0  \n",
       "2                             0.0  0.0  0.0  0.0  0.0  \n",
       "3                             0.0  0.0  0.0  0.0  0.0  \n",
       "4                             0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 120018 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformation en matrice dense pour visualiser\n",
    "X_tfidf_train_dense = pd.DataFrame(X_tfidf_train.todense(), columns=feature_names_tfidf)\n",
    "X_tfidf_test_dense = pd.DataFrame(X_tfidf_test.todense(), columns=feature_names_tfidf)\n",
    "\n",
    "X_tfidf_train_dense.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BShvQg60Clzx",
   "metadata": {
    "id": "BShvQg60Clzx"
   },
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gtRsCOhgPHK1",
   "metadata": {
    "id": "gtRsCOhgPHK1"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLe noyau s’est bloqué lors de l’exécution du code dans une cellule active ou une cellule précédente. \n",
      "\u001b[1;31mVeuillez vérifier le code dans la ou les cellules pour identifier une cause possible de l’échec. \n",
      "\u001b[1;31mCliquez <a href='https://aka.ms/vscodeJupyterKernelCrash'>ici</a> pour plus d’informations. \n",
      "\u001b[1;31mPour plus d’informations, consultez Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "def display_scree_plot(pca):\n",
    "    fig=plt.figure(figsize=(8,8))\n",
    "    scree = pca.explained_variance_ratio_*100\n",
    "    plt.bar(np.arange(len(scree))+1, scree)\n",
    "    plt.plot(np.arange(len(scree))+1, scree.cumsum(),c=\"red\",marker='o')\n",
    "    plt.xlabel(\"rang de l'axe d'inertie\")\n",
    "    plt.ylabel(\"pourcentage d'inertie\")\n",
    "    plt.title(\"Eboulis des valeurs propres\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KTsTu4uvCm4D",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "id": "KTsTu4uvCm4D",
    "outputId": "22e3560c-2bb4-4877-aa01-45a170c48220"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def pca_transformation(train , test):\n",
    "    scaler = StandardScaler()\n",
    "    train = scaler.fit_transform(train)\n",
    "    test = scaler.transform(test)\n",
    "    n_comp = train.shape[1]\n",
    "    pca = PCA(n_components=n_comp)\n",
    "    pca.fit(train)\n",
    "    display_scree_plot(pca)\n",
    "    pca = PCA(n_components=0.8, random_state=42)\n",
    "    pca.fit(train)\n",
    "    train_pca = pca.transform(train)\n",
    "    test_pca = pca.transform(test)\n",
    "    print(\"\\nNous conservons {} composantes principales pour garder 80% d'inertie\".format(pca.components_.shape[0]))\n",
    "    return train_pca, test_pca, pca\n",
    "\n",
    "X_train_tfidf_pca, X_test_tfidf_pca, pca_tfidf = pca_transformation(X_tfidf_train_dense, X_tfidf_test_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IDCCeaUjCq8Z",
   "metadata": {
    "id": "IDCCeaUjCq8Z"
   },
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_M7vtGKnCr2S",
   "metadata": {
    "id": "_M7vtGKnCr2S"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "def plot_top_words(model, feature_names,\n",
    "                   n_top_words, nb_topic_plot, title):\n",
    "    \"\"\"Function for displaying the plots of the\n",
    "    best x words representative of the categories of NMF.\n",
    "\n",
    "    Parameters\n",
    "    ----------------------------------------\n",
    "    model : NMF model\n",
    "        Fitted model of NMF to plot\n",
    "    feature_names : array\n",
    "        Categories result of the vectorizer (TFIDF ...)\n",
    "    n_top_words : int\n",
    "        Number of words for each topic.\n",
    "    title : string\n",
    "        Title of the plot.\n",
    "    ----------------------------------------\n",
    "    \"\"\"\n",
    "    rows = int(nb_topic_plot/6)\n",
    "    fig, axes = plt.subplots(rows, 6,\n",
    "                             figsize=(30, rows*10),\n",
    "                             sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        if(topic_idx < nb_topic_plot):\n",
    "            top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "            top_features = [feature_names[i] for i in top_features_ind]\n",
    "            weights = topic[top_features_ind]\n",
    "\n",
    "            ax = axes[topic_idx]\n",
    "            bartopic = ax.barh(top_features, weights, height=0.7)\n",
    "            bartopic[0].set_color('#f48023')\n",
    "            ax.set_title(f'Topic {topic_idx +1}',\n",
    "                         fontdict={'fontsize': 30})\n",
    "            ax.invert_yaxis()\n",
    "            ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "            for i in 'top right left'.split():\n",
    "                ax.spines[i].set_visible(False)\n",
    "            fig.suptitle(title, fontsize=36, color=\"#641E16\")\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()\n",
    "\n",
    "  # Define number of topics to test\n",
    "n_topics = 12\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"Start NMF fitting on Full_doc ...\")\n",
    "print(\"-\" * 50)\n",
    "# start_time = time.time()\n",
    "# Initializing the NMF\n",
    "full_nmf = NMF(n_components=n_topics,\n",
    "               init='nndsvd',\n",
    "               random_state=8)\n",
    "\n",
    "# Fit NMF on Body vectorized\n",
    "full_nmf.fit(X_tfidf_train_dense)\n",
    "\n",
    "# exec_time = time.time() - start_time\n",
    "print(\"End of training :\")\n",
    "# print(\"Execution time : {:.2f}s\".format(exec_time))\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Plot the 6 first topics\n",
    "ff_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "plot_top_words(full_nmf, ff_feature_names, 20, 6,\n",
    "               'Topics in NMF model for Full_doc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xOBlk2faCxJF",
   "metadata": {
    "id": "xOBlk2faCxJF"
   },
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3-LcbbAKCx7J",
   "metadata": {
    "id": "3-LcbbAKCx7J"
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "# Dictionnaire et corpus\n",
    "common_dictionary = corpora.Dictionary(corpus_tokenized)\n",
    "common_dictionary.filter_extremes(no_below=1000)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in corpus_tokenized]\n",
    "\n",
    "coherences = []\n",
    "topic_range = list(range(2, 20))\n",
    "\n",
    "for n_topics in topic_range:\n",
    "    lda_model = LdaMulticore(corpus=common_corpus,\n",
    "                             id2word=common_dictionary,\n",
    "                             num_topics=n_topics,\n",
    "                             passes=5,\n",
    "                             chunksize=2000,\n",
    "                             random_state=42,\n",
    "                             workers=7,\n",
    "                             per_word_topics=False)\n",
    "\n",
    "    # Cohérence directement depuis le modèle\n",
    "    cm = CoherenceModel(model=lda_model, texts=corpus_tokenized, dictionary=common_dictionary, coherence='c_v')\n",
    "    coherence_score = cm.get_coherence()\n",
    "    coherences.append(coherence_score)\n",
    "    print(f\"{n_topics} topics → coherence: {coherence_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WQHbWatWC1D0",
   "metadata": {
    "id": "WQHbWatWC1D0"
   },
   "outputs": [],
   "source": [
    "best_index = coherences.index(max(coherences))\n",
    "best_num_topics = topic_range[best_index]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(topic_range, coherences, marker='o')\n",
    "ax.axvline(x=best_num_topics, color='g', alpha=.8,\n",
    "           linestyle='dashdot', label='Best param')\n",
    "plt.xlabel(\"Nombre de topics\")\n",
    "plt.ylabel(\"Score de cohérence (c_v)\")\n",
    "plt.title(\"Évaluation de la cohérence des topics LDA\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6i0o8xKC2np",
   "metadata": {
    "id": "c6i0o8xKC2np"
   },
   "outputs": [],
   "source": [
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "import gensim.corpora as corpora\n",
    "from gensim import models\n",
    "from gensim.utils import simple_preprocess\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "\n",
    "best_lda_model = LdaMulticore(corpus=common_corpus,\n",
    "                              id2word=common_dictionary,\n",
    "                              num_topics=best_num_topics,\n",
    "                              per_word_topics=True,\n",
    "                              passes=10,\n",
    "                              workers=7)\n",
    "\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "gensimvis.prepare(best_lda_model, common_corpus, common_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mlmmS-lxC9Hl",
   "metadata": {
    "id": "mlmmS-lxC9Hl"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "def map_topics_to_tags(lda_model, y_train, corpus, top_n_docs=10):\n",
    "    topic_to_tags = {}\n",
    "    all_topic_distributions = [lda_model.get_document_topics(doc, minimum_probability=0.0) for doc in corpus]\n",
    "    num_topics = lda_model.num_topics\n",
    "\n",
    "    for topic_idx in range(num_topics):\n",
    "        topic_probs = [(doc_idx, dist[topic_idx][1]) for doc_idx, dist in enumerate(all_topic_distributions)\n",
    "                       if topic_idx < len(dist)]\n",
    "        topic_docs_sorted = sorted(topic_probs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        top_doc_indices = [doc_idx for doc_idx, _ in topic_docs_sorted[:top_n_docs]]\n",
    "\n",
    "        tags = set()\n",
    "        for doc_idx in top_doc_indices:\n",
    "            if doc_idx >= len(y_train):\n",
    "                continue  # Ignore les indices hors limites\n",
    "\n",
    "            doc_tags = y_train.iloc[doc_idx]\n",
    "            if isinstance(doc_tags, str):\n",
    "                import ast\n",
    "                doc_tags = ast.literal_eval(doc_tags)\n",
    "            tags.update(doc_tags)\n",
    "\n",
    "        topic_to_tags[topic_idx] = tags\n",
    "\n",
    "    return topic_to_tags\n",
    "\n",
    "topic_to_tags = map_topics_to_tags(best_lda_model, y_train, common_corpus)\n",
    "\n",
    "for topic, tags in topic_to_tags.items():\n",
    "    print(f\"Topic {topic}: {tags}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pPZk-0_qDBaD",
   "metadata": {
    "id": "pPZk-0_qDBaD"
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "metrics_df = pd.DataFrame()\n",
    "\n",
    "mlflow.end_run()\n",
    "\n",
    "mlflow.set_experiment(\"LDA_Topic_Modeling\")\n",
    "\n",
    "with mlflow.start_run(run_name=f\"LDA_{best_num_topics}_topics\"):\n",
    "\n",
    "  mlflow.log_param(\"num_topics\", best_num_topics)\n",
    "  mlflow.log_param(\"passes\", 10)\n",
    "  mlflow.log_param(\"workers\", 7)\n",
    "  mlflow.log_param(\"top_n_docs\", 10)\n",
    "  mlflow.log_param(\"top_n_topics\", 3)\n",
    "\n",
    "\n",
    "  # --- 1. Préparer le corpus test pour LDA ---\n",
    "  # Tokenisation des docs test (même méthode que train)\n",
    "  corpus_test_tokenized = X_test['Merged_doc'].dropna().apply(word_tokenize).tolist()\n",
    "\n",
    "  # Passage en BoW avec le dictionnaire issu du train\n",
    "  common_corpus_test = [common_dictionary.doc2bow(text) for text in corpus_test_tokenized]\n",
    "\n",
    "  # --- 2. Prédiction des tags à partir de la LDA et mapping topic->tags ---\n",
    "\n",
    "  def predict_tags_from_lda(lda_model, corpus_bow_test, topic_to_tags, top_n_topics=3):\n",
    "      all_predicted_tags = []\n",
    "      for bow in corpus_bow_test:\n",
    "          # Obtenir la distribution des topics pour un doc (minimum_probability=0 pour avoir toutes)\n",
    "          doc_topics = lda_model.get_document_topics(bow, minimum_probability=0)\n",
    "          # Trier par probabilité décroissante\n",
    "          doc_topics_sorted = sorted(doc_topics, key=lambda x: x[1], reverse=True)\n",
    "          # Prendre les N topics les plus probables\n",
    "          top_topics = [topic for topic, prob in doc_topics_sorted[:top_n_topics]]\n",
    "\n",
    "          # Récupérer tous les tags associés à ces topics\n",
    "          predicted_tags = set()\n",
    "          for topic in top_topics:\n",
    "              predicted_tags.update(topic_to_tags.get(topic, []))\n",
    "          all_predicted_tags.append(list(predicted_tags))\n",
    "      return all_predicted_tags\n",
    "\n",
    "  # --- 3. Parsing des tags test (si c’est des chaînes de caractères) ---\n",
    "  y_test_parsed = y_test.apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "  # --- 4. Prédictions ---\n",
    "  y_pred_tags = predict_tags_from_lda(best_lda_model, common_corpus_test, topic_to_tags)\n",
    "\n",
    "  # --- 5. Binarisation des tags ---\n",
    "  # On fit sur la totalité des tags observés dans train + test (important pour cohérence)\n",
    "  all_tags = list(y_train) + list(y_test_parsed)\n",
    "  mlb.fit(all_tags)\n",
    "\n",
    "  Y_test_bin = mlb.transform(y_test_parsed)\n",
    "  Y_pred_bin = mlb.transform(y_pred_tags)\n",
    "\n",
    "  metrics_df = metric_score(\"LDA\", Y_test_bin, Y_pred_bin, metrics_df)\n",
    "\n",
    "  mlflow.log_metric(\"accuracy\", metrics_df.loc[\"Accuracy\", \"LDA\"])\n",
    "  mlflow.log_metric(\"f1_score\", metrics_df.loc[\"F1 score\", \"LDA\"])\n",
    "  mlflow.log_metric(\"jaccard\", metrics_df.loc[\"Jaccard\", \"LDA\"])\n",
    "  mlflow.log_metric(\"recall\", metrics_df.loc[\"Recall\", \"LDA\"])\n",
    "  mlflow.log_metric(\"precision\", metrics_df.loc[\"Precision\", \"LDA\"])\n",
    "\n",
    "mlflow.end_run()\n",
    "\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WVoWZVcLG4le",
   "metadata": {
    "id": "WVoWZVcLG4le"
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "    model_path = os.path.join(tmp_dir, \"lda_model\")\n",
    "    best_lda_model.save(model_path)\n",
    "    mlflow.log_artifact(model_path, artifact_path=\"lda_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mCABz2wzDlPG",
   "metadata": {
    "id": "mCABz2wzDlPG"
   },
   "source": [
    "# LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UDL_6KghDnWG",
   "metadata": {
    "id": "UDL_6KghDnWG"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import ast\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "mlflow.end_run()\n",
    "\n",
    "# Binarisation des labels\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y_train_bin = mlb.fit_transform(y_train.apply(ast.literal_eval))\n",
    "Y_test_bin = mlb.transform(y_test.apply(ast.literal_eval))\n",
    "\n",
    "print(f\"Nombre de classes dans mlb: {len(mlb.classes_)}\")\n",
    "print(f\"Shape Y_train_bin: {Y_train_bin.shape}\")\n",
    "print(f\"Shape Y_test_bin: {Y_test_bin.shape}\")\n",
    "\n",
    "# Paramètres pour la recherche\n",
    "param_logit = {\n",
    "    \"estimator__C\": [100, 10, 1.0, 0.1],\n",
    "    \"estimator__penalty\": [\"l1\", \"l2\"],\n",
    "    \"estimator__dual\": [False],\n",
    "    \"estimator__solver\": [\"liblinear\"]\n",
    "}\n",
    "\n",
    "# Début du run MLflow\n",
    "mlflow.set_experiment(\"Multilabel_TFIDF_LogReg\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"GridSearch_LogReg\", nested=True):\n",
    "\n",
    "    # GridSearch\n",
    "    multi_logit_cv = GridSearchCV(\n",
    "        OneVsRestClassifier(LogisticRegression()),\n",
    "        param_grid=param_logit,\n",
    "        n_jobs=-1,\n",
    "        cv=5,\n",
    "        scoring=\"f1_weighted\",\n",
    "        return_train_score=True,\n",
    "        refit=True,\n",
    "        verbose=3\n",
    "    )\n",
    "    multi_logit_cv.fit(X_tfidf_train_dense, Y_train_bin)\n",
    "\n",
    "    # Log des hyperparamètres optimaux\n",
    "    mlflow.log_params(multi_logit_cv.best_params_)\n",
    "\n",
    "    # Prédictions\n",
    "    y_test_predicted_labels_tfidf = multi_logit_cv.predict(X_tfidf_test_dense)\n",
    "\n",
    "    # Évaluation\n",
    "    print(f\"Shape des prédictions: {y_test_predicted_labels_tfidf.shape}\")\n",
    "    print(f\"Nombre de 1 dans les prédictions: {y_test_predicted_labels_tfidf.sum()}\")\n",
    "    print(f\"Pourcentage de 1: {y_test_predicted_labels_tfidf.mean()*100:.2f}%\")\n",
    "\n",
    "    # Métriques\n",
    "    metrics_df = metric_score(\"Log_Reg\", Y_test_bin, y_test_predicted_labels_tfidf, metrics_df)\n",
    "    mlflow.log_metric(\"accuracy\", metrics_df.loc[\"Accuracy\", \"Log_Reg\"])\n",
    "    mlflow.log_metric(\"f1_score\", metrics_df.loc[\"F1 score\", \"Log_Reg\"])\n",
    "    mlflow.log_metric(\"jaccard\", metrics_df.loc[\"Jaccard\", \"Log_Reg\"])\n",
    "    mlflow.log_metric(\"recall\", metrics_df.loc[\"Recall\", \"Log_Reg\"])\n",
    "    mlflow.log_metric(\"precision\", metrics_df.loc[\"Precision\", \"Log_Reg\"])\n",
    "\n",
    "    # Log du modèle\n",
    "    mlflow.sklearn.log_model(multi_logit_cv.best_estimator_, \"model\")\n",
    "\n",
    "    # Optionnel : log d'artefacts comme CSV\n",
    "    logit_cv_results = pd.DataFrame.from_dict(multi_logit_cv.cv_results_)\n",
    "    logit_cv_results.to_csv(\"logit_cv_results.csv\", index=False)\n",
    "    mlflow.log_artifact(\"logit_cv_results.csv\")\n",
    "\n",
    "    # Print comparaison prédictions\n",
    "    y_test_pred_inversed = mlb.inverse_transform(y_test_predicted_labels_tfidf)\n",
    "    y_test_inversed = mlb.inverse_transform(Y_test_bin)\n",
    "    print(\"Predicted:\", y_test_pred_inversed[:10])\n",
    "    print(\"True:\", y_test_inversed[:10])\n",
    "\n",
    "mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TwjsV3LIDrS7",
   "metadata": {
    "id": "TwjsV3LIDrS7"
   },
   "outputs": [],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dwv5VzeLDsXw",
   "metadata": {
    "id": "dwv5VzeLDsXw"
   },
   "source": [
    "# RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soV5ksa2Dxnr",
   "metadata": {
    "id": "soV5ksa2Dxnr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "\n",
    "mlflow.end_run()\n",
    "\n",
    "param_rfc = {\n",
    "    \"estimator__max_depth\": [5, 25, 50],\n",
    "    \"estimator__min_samples_leaf\": [1, 5, 10],\n",
    "    \"estimator__class_weight\": [\"balanced\"]\n",
    "}\n",
    "\n",
    "mlflow.set_experiment(\"Multilabel_TFIDF_RF\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"GridSearch_RF\", nested=True):\n",
    "\n",
    "    param_rfc = {\n",
    "    \"estimator__max_depth\": [5, 25, 50],\n",
    "    \"estimator__min_samples_leaf\": [1, 5, 10],\n",
    "    \"estimator__class_weight\": [\"balanced\"]\n",
    "}\n",
    "\n",
    "mlflow.set_experiment(\"Multilabel_TFIDF_RF\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"GridSearch_RF\", nested=True):\n",
    "\n",
    "    multi_rfc_cv = GridSearchCV(\n",
    "        OneVsRestClassifier(RandomForestClassifier()),\n",
    "        param_grid=param_rfc,\n",
    "        n_jobs=-1,\n",
    "        cv=2,\n",
    "        scoring=\"f1_weighted\",\n",
    "        return_train_score=True,\n",
    "        refit=True,\n",
    "        verbose=3\n",
    "    )\n",
    "\n",
    "    multi_rfc_cv.fit(X_tfidf_train_dense, Y_train_bin)\n",
    "\n",
    "    # Log des meilleurs hyperparamètres\n",
    "    mlflow.log_params(multi_rfc_cv.bestparams)\n",
    "\n",
    "    # Résultats de validation croisée\n",
    "    rfc_cv_results = pd.DataFrame.from_dict(multi_rfc_cv.cvresults)\n",
    "    rfc_cv_results.to_csv(\"rfc_cv_results.csv\", index=False)\n",
    "    mlflow.log_artifact(\"rfc_cv_results.csv\")\n",
    "\n",
    "    print(\"-\"*50)\n",
    "    print(\"Best params for RandomForestClassifier\")\n",
    "    print(\"-\"*50)\n",
    "    print(multi_rfc_cv.bestparams)\n",
    "\n",
    "    y_test_predicted_labels_tfidf_rfc = multi_rfc_cv.predict(X_tfidf_test_dense)\n",
    "\n",
    "    print(f\"Shape des prédictions: {y_test_predicted_labels_tfidf_rfc.shape}\")\n",
    "    print(f\"Nombre de 1 dans les prédictions: {y_test_predicted_labels_tfidf_rfc.sum()}\")\n",
    "    print(f\"Pourcentage de 1: {y_test_predicted_labels_tfidf_rfc.mean()*100:.2f}%\")\n",
    "\n",
    "    # Métriques\n",
    "    metrics_df = metric_score(\"RF\", Y_test_bin, y_test_predicted_labels_tfidf_rfc, metrics_df)\n",
    "    mlflow.log_metric(\"accuracy\", metrics_df.loc[\"Accuracy\", \"RF\"])\n",
    "    mlflow.log_metric(\"f1_score\", metrics_df.loc[\"F1 score\", \"RF\"])\n",
    "    mlflow.log_metric(\"jaccard\", metrics_df.loc[\"Jaccard\", \"RF\"])\n",
    "    mlflow.log_metric(\"recall\", metrics_df.loc[\"Recall\", \"RF\"])\n",
    "    mlflow.log_metric(\"precision\", metrics_df.loc[\"Precision\", \"RF\"])\n",
    "\n",
    "    # Log du modèle\n",
    "    mlflow.sklearn.log_model(multi_rfc_cv.bestestimator, \"model\")\n",
    "\n",
    "    # Affichage des 10 premières prédictions\n",
    "    y_test_pred_inversed = mlb.inverse_transform(y_test_predicted_labels_tfidf_rfc)\n",
    "    y_test_inversed = mlb.inverse_transform(Y_test_bin)\n",
    "\n",
    "    print(\"-\"*50)\n",
    "    print(\"Print 10 first predicted Tags vs true Tags\")\n",
    "    print(\"-\"*50)\n",
    "    print(\"Predicted:\", y_test_pred_inversed[:10])\n",
    "    print(\"True:\", y_test_inversed[:10])\n",
    "\n",
    "mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cDKgl_gOEmyB",
   "metadata": {
    "id": "cDKgl_gOEmyB"
   },
   "outputs": [],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jX_NLuGMhqzG",
   "metadata": {
    "id": "jX_NLuGMhqzG"
   },
   "source": [
    "# WORD2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LG4pZeRmHR5z",
   "metadata": {
    "id": "LG4pZeRmHR5z"
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import os\n",
    "import json\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Tokenisation\n",
    "X_train_tok = X_train['Merged_doc'].dropna().apply(word_tokenize).tolist()\n",
    "X_test_tok = X_test['Merged_doc'].dropna().apply(word_tokenize).tolist()\n",
    "\n",
    "# === Début du run MLflow ===\n",
    "mlflow.set_experiment(\"Word2Vec_Classifier\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"W2V + LogisticRegression\", nested=True):\n",
    "\n",
    "    # Hyperparams\n",
    "    vector_size = 100\n",
    "    window = 5\n",
    "    min_count = 2\n",
    "    sg = 1  # skip-gram\n",
    "\n",
    "    # Log des params\n",
    "    mlflow.log_params({\n",
    "        \"vector_size\": vector_size,\n",
    "        \"window\": window,\n",
    "        \"min_count\": min_count,\n",
    "        \"sg\": sg,\n",
    "        \"classifier\": \"LogisticRegression\"\n",
    "    })\n",
    "\n",
    "    # Entraînement Word2Vec\n",
    "    model_w2v = Word2Vec(sentences=X_train_tok, vector_size=vector_size,\n",
    "                         window=window, min_count=min_count, workers=4, sg=sg)\n",
    "\n",
    "    # Sauvegarde du modèle Word2Vec\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        w2v_path = os.path.join(tmpdir, \"word2vec.model\")\n",
    "        model_w2v.save(w2v_path)\n",
    "        mlflow.log_artifact(w2v_path, artifact_path=\"w2v_model\")\n",
    "\n",
    "    # Vectorisation\n",
    "    def vectorize_doc(doc_tokens, model):\n",
    "        vectors = [model.wv[word] for word in doc_tokens if word in model.wv]\n",
    "        return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "    X_train_vec = np.array([vectorize_doc(doc, model_w2v) for doc in X_train_tok])\n",
    "    X_test_vec = np.array([vectorize_doc(doc, model_w2v) for doc in X_test_tok])\n",
    "\n",
    "    # Classification\n",
    "    clf = OneVsRestClassifier(LogisticRegression(max_iter=1000))\n",
    "    clf.fit(X_train_vec, Y_train_bin)\n",
    "\n",
    "    Y_pred_bin = clf.predict(X_test_vec)\n",
    "\n",
    "    # Logging modèle Sklearn\n",
    "    mlflow.sklearn.log_model(clf, \"logistic_model\")\n",
    "\n",
    "    # Métriques\n",
    "    metrics_df = metric_score(\"Word2Vec\", Y_test_bin, Y_pred_bin, metrics_df)\n",
    "\n",
    "    mlflow.log_metric(\"accuracy\", metrics_df.loc[\"Accuracy\", \"Word2Vec\"])\n",
    "    mlflow.log_metric(\"f1_score\", metrics_df.loc[\"F1 score\", \"Word2Vec\"])\n",
    "    mlflow.log_metric(\"jaccard\", metrics_df.loc[\"Jaccard\", \"Word2Vec\"])\n",
    "    mlflow.log_metric(\"recall\", metrics_df.loc[\"Recall\", \"Word2Vec\"])\n",
    "    mlflow.log_metric(\"precision\", metrics_df.loc[\"Precision\", \"Word2Vec\"])\n",
    "\n",
    "mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typtCp-jHgk3",
   "metadata": {
    "id": "typtCp-jHgk3"
   },
   "outputs": [],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac727d8",
   "metadata": {
    "id": "fac727d8"
   },
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4VrRkN1rEVlD",
   "metadata": {
    "id": "4VrRkN1rEVlD"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoConfig, TFAutoModel\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xs9gmh2IEYai",
   "metadata": {
    "id": "Xs9gmh2IEYai"
   },
   "outputs": [],
   "source": [
    "# Fonction de préparation des sentences\n",
    "def bert_inp_fct(sentences, bert_tokenizer, max_length) :\n",
    "    input_ids=[]\n",
    "    token_type_ids = []\n",
    "    attention_mask=[]\n",
    "    bert_inp_tot = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        bert_inp = bert_tokenizer.encode_plus(sent,\n",
    "                                              add_special_tokens = True,\n",
    "                                              max_length = max_length,\n",
    "                                              padding='max_length',\n",
    "                                              return_attention_mask = True,\n",
    "                                              return_token_type_ids=True,\n",
    "                                              truncation=True,\n",
    "                                              return_tensors=\"tf\")\n",
    "\n",
    "        input_ids.append(bert_inp['input_ids'][0])\n",
    "        token_type_ids.append(bert_inp['token_type_ids'][0])\n",
    "        attention_mask.append(bert_inp['attention_mask'][0])\n",
    "        bert_inp_tot.append((bert_inp['input_ids'][0],\n",
    "                             bert_inp['token_type_ids'][0],\n",
    "                             bert_inp['attention_mask'][0]))\n",
    "\n",
    "    input_ids = np.asarray(input_ids)\n",
    "    token_type_ids = np.asarray(token_type_ids)\n",
    "    attention_mask = np.array(attention_mask)\n",
    "\n",
    "    return input_ids, token_type_ids, attention_mask, bert_inp_tot\n",
    "\n",
    "\n",
    "# Fonction de création des features\n",
    "def feature_BERT_fct(model, model_type, sentences, max_length, b_size, mode='HF') :\n",
    "    batch_size = b_size\n",
    "    batch_size_pred = b_size\n",
    "    bert_tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "    time1 = time.time()\n",
    "\n",
    "    # Ensure sentences is a list of strings\n",
    "    if isinstance(sentences, pd.DataFrame):\n",
    "        sentences = sentences['Merged_doc'].tolist()\n",
    "    elif not isinstance(sentences, list):\n",
    "        raise TypeError(\"Input 'sentences' must be a list or a pandas DataFrame.\")\n",
    "\n",
    "\n",
    "    num_batches = (len(sentences) + batch_size - 1) // batch_size # Calculate number of batches including the last partial batch\n",
    "\n",
    "    for step in range(num_batches) :\n",
    "        idx = step*batch_size\n",
    "        # Handle the last batch which might be smaller than batch_size\n",
    "        current_batch_sentences = sentences[idx:min(idx+batch_size, len(sentences))]\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask, bert_inp_tot = bert_inp_fct(current_batch_sentences,\n",
    "                                                                      bert_tokenizer, max_length)\n",
    "\n",
    "        if mode=='HF' :    # Bert HuggingFace\n",
    "            outputs = model.predict([input_ids, attention_mask, token_type_ids], batch_size=batch_size_pred)\n",
    "            last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        if mode=='TFhub' : # Bert Tensorflow Hub\n",
    "            text_preprocessed = {\"input_word_ids\" : input_ids,\n",
    "                                 \"input_mask\" : attention_mask,\n",
    "                                 \"input_type_ids\" : token_type_ids}\n",
    "            outputs = model(text_preprocessed)\n",
    "            last_hidden_states = outputs['sequence_output']\n",
    "\n",
    "        if step == 0 :\n",
    "            last_hidden_states_tot = last_hidden_states\n",
    "        else :\n",
    "            last_hidden_states_tot = np.concatenate((last_hidden_states_tot,last_hidden_states))\n",
    "\n",
    "    features_bert = np.array(last_hidden_states_tot).mean(axis=1)\n",
    "\n",
    "    time2 = np.round(time.time() - time1,0)\n",
    "    print(\"temps traitement : \", time2)\n",
    "\n",
    "    return features_bert, last_hidden_states_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3IISJoO6Ejg_",
   "metadata": {
    "id": "3IISJoO6Ejg_"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModel\n",
    "\n",
    "max_length = 32\n",
    "batch_size = 10\n",
    "model_type = 'bert-base-uncased'\n",
    "\n",
    "model = TFAutoModel.from_pretrained(model_type)\n",
    "sentences_train = X_train\n",
    "sentences_test = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3877LB7Bjvnr",
   "metadata": {
    "id": "3877LB7Bjvnr"
   },
   "outputs": [],
   "source": [
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nMQ3J0M9EoLs",
   "metadata": {
    "id": "nMQ3J0M9EoLs"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_bert_train, last_hidden_hf_train = feature_BERT_fct(model, model_type,\n",
    "                                                   X_train, max_length,\n",
    "                                                   batch_size, mode='HF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pEytMA0mEpWJ",
   "metadata": {
    "id": "pEytMA0mEpWJ"
   },
   "outputs": [],
   "source": [
    "X_bert_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Wxbmv6WqErSF",
   "metadata": {
    "id": "Wxbmv6WqErSF"
   },
   "outputs": [],
   "source": [
    "X_bert_test, last_hidden_hf_test = feature_BERT_fct(model, model_type,\n",
    "                                                   X_test, max_length,\n",
    "                                                   batch_size, mode='HF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bURLgrQyE1pa",
   "metadata": {
    "id": "bURLgrQyE1pa"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X_train_bert_dense = X_bert_train\n",
    "X_test_bert_dense = X_bert_test\n",
    "\n",
    "pca = PCA(n_components=100, random_state=42)\n",
    "X_train_bert_pca = pca.fit_transform(X_train_bert_dense)\n",
    "X_test_bert_pca = pca.transform(X_test_bert_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ozeNIetxGfYI",
   "metadata": {
    "id": "ozeNIetxGfYI"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "mlflow.set_experiment(\"Multilabel_BERT\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"LogReg_BERT\", nested=True):\n",
    "    classifier = OneVsRestClassifier(LogisticRegression(max_iter=1000))\n",
    "    classifier.fit(X_train_bert_pca, Y_train_bin)\n",
    "\n",
    "    # Predict\n",
    "    Y_pred_bin = classifier.predict(X_test_bert_pca)\n",
    "\n",
    "    # Metrics\n",
    "    metrics_df = metric_score(\"Bert\", Y_test_bin, Y_pred_bin, metrics_df)\n",
    "\n",
    "    # Log params\n",
    "    mlflow.log_param(\"BERT_model\", model_type)\n",
    "    mlflow.log_param(\"max_length\", max_length)\n",
    "    mlflow.log_param(\"pca_components\", 100)\n",
    "\n",
    "    # Log metrics (juste après calcul)\n",
    "    mlflow.log_metric(\"accuracy\", metrics_df.loc[\"Accuracy\", \"Bert\"])\n",
    "    mlflow.log_metric(\"f1_score\", metrics_df.loc[\"F1 score\", \"Bert\"])\n",
    "    mlflow.log_metric(\"jaccard\", metrics_df.loc[\"Jaccard\", \"Bert\"])\n",
    "    mlflow.log_metric(\"recall\", metrics_df.loc[\"Recall\", \"Bert\"])\n",
    "    mlflow.log_metric(\"precision\", metrics_df.loc[\"Precision\", \"Bert\"])\n",
    "\n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(classifier, \"model\")\n",
    "\n",
    "mlflow.end_run()\n",
    "\n",
    "# Affichage final\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Su2_YjMOxy61",
   "metadata": {
    "id": "Su2_YjMOxy61"
   },
   "outputs": [],
   "source": [
    "metrics_df.to_csv('data/metrics_df_2.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3GrTJMLS-o3D",
   "metadata": {
    "id": "3GrTJMLS-o3D"
   },
   "source": [
    "#USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qifjwUL997xF",
   "metadata": {
    "id": "qifjwUL997xF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Définir le cache temporaire\n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = \"/tmp/tfhub_cache\"\n",
    "\n",
    "# Charger le modèle USE\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PDcANNu0-F_J",
   "metadata": {
    "id": "PDcANNu0-F_J"
   },
   "outputs": [],
   "source": [
    "def feature_USE_fct(sentences, batch_size):\n",
    "    features = []\n",
    "    time1 = time.time()\n",
    "\n",
    "    for step in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[step:step + batch_size]\n",
    "\n",
    "        # 🔽 Correction ici : aplatir si array 2D\n",
    "        if isinstance(batch, np.ndarray):\n",
    "            batch = batch.ravel().tolist()\n",
    "\n",
    "        feat = embed(batch)\n",
    "        features.append(feat.numpy())\n",
    "\n",
    "    features = np.vstack(features)\n",
    "    print(f\"Temps d'encodage : {np.round(time.time() - time1, 2)} sec\")\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HNRYrCmtgEQW",
   "metadata": {
    "id": "HNRYrCmtgEQW"
   },
   "outputs": [],
   "source": [
    "print(type(X_train))\n",
    "print(np.array(X_train).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1zAOQiuigKPr",
   "metadata": {
    "id": "1zAOQiuigKPr"
   },
   "outputs": [],
   "source": [
    "X_train = np.ravel(X_train).tolist()\n",
    "X_test = np.ravel(X_test).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7g95eTK1ga3R",
   "metadata": {
    "id": "7g95eTK1ga3R"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def display_scree_plot(pca):\n",
    "  fig=plt.figure(figsize=(8,8))\n",
    "  scree = pca.explained_variance_ratio_*100\n",
    "  plt.bar(np.arange(len(scree))+1, scree)\n",
    "  plt.plot(np.arange(len(scree))+1, scree.cumsum(),c=\"red\",marker='o')\n",
    "  plt.xlabel(\"rang de l'axe d'inertie\")\n",
    "  plt.ylabel(\"pourcentage d'inertie\")\n",
    "  plt.title(\"Eboulis des valeurs propres\")\n",
    "  plt.show()\n",
    "\n",
    "def pca_transformation(train , test):\n",
    "    scaler = StandardScaler()\n",
    "    train = scaler.fit_transform(train)\n",
    "    test = scaler.transform(test)\n",
    "    n_comp = train.shape[1]\n",
    "    pca = PCA(n_components=n_comp)\n",
    "    pca.fit(train)\n",
    "    display_scree_plot(pca)\n",
    "    pca = PCA(n_components=0.8, random_state=42)\n",
    "    pca.fit(train)\n",
    "    train_pca = pca.transform(train)\n",
    "    test_pca = pca.transform(test)\n",
    "    print(\"\\nNous conservons {} composantes principales pour garder 80% d'inertie\".format(pca.components_.shape[0]))\n",
    "    return train_pca, test_pca, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-y7NjV2XeXze",
   "metadata": {
    "id": "-y7NjV2XeXze"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Binarisation des étiquettes multilabels\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y_train_bin = mlb.fit_transform(y_train)\n",
    "Y_test_bin = mlb.transform(y_test)\n",
    "\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5Nw9oKlNM9vR",
   "metadata": {
    "id": "5Nw9oKlNM9vR"
   },
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"Multilabel_USE\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"LogReg_USE\", nested=True):\n",
    "    # Encodage avec USE\n",
    "    X_use_train = feature_USE_fct(X_train, batch_size)\n",
    "    X_use_test = feature_USE_fct(X_test, batch_size)\n",
    "\n",
    "    # PCA\n",
    "    X_train_use_pca, X_test_use_pca, pca_use = pca_transformation(X_use_train, X_use_test)\n",
    "    n_components = pca_use.n_components_\n",
    "\n",
    "    # Log des paramètres\n",
    "    mlflow.log_param(\"embedding\", \"USE\")\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"pca_components\", n_components)\n",
    "    mlflow.log_param(\"C\", 10)\n",
    "    mlflow.log_param(\"penalty\", \"l1\")\n",
    "    mlflow.log_param(\"solver\", \"liblinear\")\n",
    "\n",
    "    # Modèle\n",
    "    logit_use_pca = OneVsRestClassifier(\n",
    "        LogisticRegression(C=10, penalty=\"l1\", dual=False, solver=\"liblinear\"),\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    logit_use_pca.fit(X_train_use_pca, Y_train_bin)\n",
    "\n",
    "    # Prédictions\n",
    "    Y_pred_use = logit_use_pca.predict(X_test_use_pca)\n",
    "\n",
    "    metrics_df = metric_score(\"USE\", Y_test_bin, Y_pred_use, metrics_df)\n",
    "\n",
    "    # Log métriques\n",
    "    mlflow.log_metric(\"accuracy\", metrics_df.loc[\"Accuracy\", \"USE\"])\n",
    "    mlflow.log_metric(\"f1_score\", metrics_df.loc[\"F1 score\", \"USE\"])\n",
    "    mlflow.log_metric(\"jaccard\", metrics_df.loc[\"Jaccard\", \"USE\"])\n",
    "    mlflow.log_metric(\"recall\", metrics_df.loc[\"Recall\", \"USE\"])\n",
    "    mlflow.log_metric(\"precision\", metrics_df.loc[\"Precision\", \"USE\"])\n",
    "\n",
    "    # Log du modèle\n",
    "    mlflow.sklearn.log_model(logit_use_pca, \"model\")\n",
    "\n",
    "mlflow.end_run()\n",
    "\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hKAGivk1VHPP",
   "metadata": {
    "id": "hKAGivk1VHPP"
   },
   "outputs": [],
   "source": [
    "metrics_df.to_csv('data/metrics_df_3.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311fb29a-e9d9-49eb-b7e7-0b410cac38e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow ui --port 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f6780c-0865-4bac-89b7-8a84e2c354a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
